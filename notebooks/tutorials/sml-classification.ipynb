{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: install Qiskit (runs automatically in Colab, no-op in Binder)\n",
    "!pip install -q qiskit qiskit-aer qiskit-ibm-runtime pylatexenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional dependencies for this notebook\n",
    "!pip install -q imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1e3ec",
   "metadata": {},
   "source": "# Hybrid quantum-enhanced ensemble classification (daloy ng grid stability workflow)\n\n*Pagtatantya ng paggamit: 20 minuto sa QPU time para sa bawat gawain sa isang Eagle r3 processor. (PAALALA: Ito ay pagtatantya lamang. Maaaring mag-iba ang inyong runtime.)*\n## Background\nAng tutorial na ito ay nagpapakita ng hybrid quantum–classical workflow na nagpapahusay sa isang klasikal na ensemble gamit ang quantum optimization step. Ginagamit ang \"Singularity Machine Learning – Classification\" ng Multiverse Computing (isang Qiskit Function), magsasanay tayo ng grupo ng konbensyonal na mga learner (halimbawa, decision trees, k-NN, logistic regression) at pagkatapos ay pahusayin ang grupong iyon gamit ang quantum layer upang mapabuti ang diversity at generalization. Ang layunin ay praktikal: sa isang tunay na gawain ng paghula ng katatagan ng grid, ihahambing natin ang malakas na klasikal na baseline sa isang quantum-optimized na alternatibo sa ilalim ng parehong data splits, upang makita ninyo kung saan nakakatulong ang quantum step at ano ang gastos nito.\n\nBakit ito mahalaga: ang pagpili ng magandang subset mula sa maraming mahinang learner ay isang kombinatoryal na problema na mabilis na lumalaki sa laki ng ensemble. Ang mga klasikal na heuristic tulad ng boosting, bagging, at stacking ay mahusay sa katamtamang sukat ngunit maaaring mahirapan na tuklasin nang epektibo ang malalaking, redundant na mga library ng mga modelo. Ang function ay nagsasama ng mga quantum algorithm - partikular ang QAOA (at opsyonal na VQE sa ibang mga configuration) - upang mas epektibong magsaliksik sa espasyong iyon pagkatapos masanay ang mga klasikal na learner, na nagpapataas ng pagkakataon na makahanap ng compact, diverse na subset na mas mahusay sa generalization.\n\nMahalagang tandaan, ang sukat ng data ay hindi limitado ng mga qubit. Ang mabigat na gawain sa data — preprocessing, pagsasanay ng learner pool, at evaluation — ay nananatiling klasikal at kayang humawak ng milyun-milyong halimbawa. Ang mga qubit ay tumutukoy lamang ng laki ng ensemble na ginagamit sa quantum selection step. Ang paghihiwalayan na ito ang nagpapahintulot ng approach sa kasalukuyang hardware: pinapanatili ninyo ang pamilyar na scikit-learn workflow para sa data at model training habang tinatawag ang quantum step sa pamamagitan ng malinis na action interface sa Qiskit Functions.\n\nSa praktis, bagaman maaaring magbigay ng iba't ibang uri ng learner sa ensemble (hal., decision trees, logistic regression, o k-NN), ang Decision Trees ay karaniwang pinakamahusay. Ang optimizer ay patuloy na pumipili ng mas malakas na miyembro ng ensemble—kapag ibinigay ang heterogeneous na mga learner, ang mas mahinang mga modelo tulad ng linear regressors ay karaniwang tinatanggal pabor sa mas expressive na mga ito tulad ng Decision Trees.\n\nAng inyong gagawin dito: maghanda at balansehin ang grid-stability dataset; magtatag ng klasikal na AdaBoost baseline; magpatakbo ng ilang quantum configuration na nag-iiba ng ensemble width at regularization; magsagawa sa IBM&reg; simulator o QPU sa pamamagitan ng Qiskit Serverless; at ihambing ang accuracy, precision, recall, at F1 sa lahat ng runs. Sa daan, gagamitin ninyo ang action pattern ng function (`create`, `fit`, `predict`, `fit_predict`, `create_fit_predict`) at mga pangunahing control:\n- Mga uri ng regularization: `onsite` (λ) para sa direktang sparsity at `alpha` para sa ratio-based trade-off sa pagitan ng interaction at onsite terms\n- Auto-regularization: itakda ang `regularization=\"auto\"` na may target selection ratio upang awtomatikong umangkop ang sparsity\n- Mga opsyon ng optimizer: simulator kontra QPU, mga pag-uulit, klasikal na optimizer at mga opsyon nito, lalim ng transpilation, at mga setting ng runtime sampler/estimator\n\nAng mga benchmark sa dokumentasyon ay nagpapakita na ang accuracy ay bumubuti habang tumataas ang bilang ng mga learner (qubit) sa mahihirap na problema, at ang quantum classifier ay tumutugma o lumalampas sa maihahambing na klasikal na ensemble. Sa tutorial na ito, uulitin ninyo ang workflow mula simula hanggang dulo at susuriin kung kailan ang pagtaas ng ensemble width o paglipat sa adaptive regularization ay nagbubunga ng mas magandang F1 sa makatwirang paggamit ng resources. Ang resulta ay isang makatotohanang pananaw kung paano ang quantum optimization step ay maaaring kumplemento, sa halip na palitan, ang klasikal na ensemble learning sa tunay na mga aplikasyon.\n## Requirements\nBago magsimula sa tutorial na ito, tiyaking naka-install sa inyong Python environment ang sumusunod na mga package:\n\n- `qiskit[visualization]~=2.1.0`\n- `qiskit-serverless~=0.24.0`\n- `qiskit-ibm-runtime v0.40.1`\n- `qiskit-ibm-catalog~=0.8.0`\n- `scikit-learn==1.5.2`\n- `pandas>=2.0.0,<3.0.0`\n- `imbalanced-learn~=0.12.3`\n## Setup\nSa seksyong ito, magsisimula tayo ng Qiskit Serverless client at ikakarga ang Singularity Machine Learning – Classification function na ibinigay ng Multiverse Computing.\nSa Qiskit Serverless, maaari ninyong patakbuhin ang hybrid quantum–classical workflow sa IBM managed cloud infrastructure nang hindi nag-aalala sa resource management.\nKakailanganin ninyo ng IBM Quantum Platform API key at inyong cloud resource name (CRN) upang mag-authenticate at ma-access ang Qiskit Functions.\n### Download the dataset\nUpang patakbuhin ang tutorial na ito, gagamitin natin ang pre-processed na **grid stability classification dataset** na naglalaman ng mga naka-label na sensor reading ng power system.\nAng sumusunod na cell ay awtomatikong lumilikha ng kinakailangang folder structure at dini-download ang parehong training at test file nang direkta sa inyong environment gamit ang `wget`.\nKung mayroon na kayong mga file na ito locally, ang hakbang na ito ay ligtas na mag-overwrite sa kanila upang matiyak ang version consistency."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f69b77",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "8bf80006",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "55b94021",
   "metadata": {},
   "source": ""
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7db2e559",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "d4fe7ee1-21ce-445c-b151-598cd4cf9227",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a32efb3-a425-4c02-804b-65029ecffb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tutorial/grid_ 100%[===================>] 612.94K  --.-KB/s    in 0.01s   \n",
      "data_tutorial/grid_ 100%[===================>] 108.19K  --.-KB/s    in 0.006s  \n",
      "Dataset files downloaded:\n",
      "-rw-r--r-- 1 coder coder 109K Nov  8 18:50 data_tutorial/grid_stability/test.csv\n",
      "-rw-r--r-- 1 coder coder 613K Nov  8 18:50 data_tutorial/grid_stability/train.csv\n"
     ]
    }
   ],
   "source": [
    "## Download dataset for Grid Stability Classification\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "!mkdir -p data_tutorial/grid_stability\n",
    "\n",
    "# Download the training and test sets from the official Qiskit documentation repo\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/train.csv \\\n",
    "  https://raw.githubusercontent.com/Qiskit/documentation/main/datasets/tutorials/grid_stability/train.csv\n",
    "\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/test.csv \\\n",
    "  https://raw.githubusercontent.com/Qiskit/documentation/main/datasets/tutorials/grid_stability/test.csv\n",
    "\n",
    "# Check the files have been downloaded\n",
    "!echo \"Dataset files downloaded:\"\n",
    "!ls -lh data_tutorial/grid_stability/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aa84f-ab37-412c-b056-7043b73380fa",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "\n",
    "In this section, we import all Python packages and Qiskit modules used throughout the tutorial.\n",
    "These include core scientific libraries for data handling and model evaluation - such as `NumPy`, `pandas`, and `scikit-learn` - along with visualization tools and Qiskit components for running the quantum-enhanced model.\n",
    "We also import the `QiskitRuntimeService` and `QiskitFunctionsCatalog` to connect with IBM Quantum&reg; services and access the Singularity Machine Learning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c654f5-8355-4f67-b79d-c2b1c29ccc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from qiskit_ibm_catalog import QiskitFunctionsCatalog\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b389b34-02e8-4a63-ae24-0bf348647b48",
   "metadata": {},
   "source": "### Import required packages\nSa seksyong ito, iai-import natin ang lahat ng Python package at Qiskit module na gagamitin sa buong tutorial.\nKasama dito ang mga pangunahing siyentipikong library para sa data handling at model evaluation - tulad ng `NumPy`, `pandas`, at `scikit-learn` - kasama ang mga visualization tool at Qiskit component para sa pagpapatakbo ng quantum-enhanced model.\nIai-import din natin ang `QiskitRuntimeService` at `QiskitFunctionsCatalog` upang kumonekta sa IBM Quantum&reg; services at ma-access ang Singularity Machine Learning function."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68bf7df-914c-4b2a-827f-657578503750",
   "metadata": {},
   "outputs": [],
   "source": [
    "IBM_TOKEN = \"\"\n",
    "IBM_INSTANCE_TEST = \"\"\n",
    "IBM_INSTANCE_QUANTUM = \"\"\n",
    "FUNCTION_NAME = \"multiverse/singularity\"\n",
    "RANDOM_STATE: int = 123\n",
    "TRAIN_PATH = \"data_tutorial/grid_stability/train.csv\"\n",
    "TEST_PATH = \"data_tutorial/grid_stability/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709dc1c-b380-49f1-95c7-89197aa5e147",
   "metadata": {},
   "source": "### Set constant variables"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc380c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\n",
      "Catalog: <QiskitFunctionsCatalog>\n",
      "Singularity function: QiskitFunction(multiverse/singularity)\n"
     ]
    }
   ],
   "source": [
    "service = QiskitRuntimeService(\n",
    "    token=IBM_TOKEN,\n",
    "    channel=\"ibm_quantum_platform\",\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    ")\n",
    "\n",
    "backend = service.least_busy()\n",
    "catalog = QiskitFunctionsCatalog(\n",
    "    token=IBM_TOKEN,\n",
    "    instance=IBM_INSTANCE_TEST,\n",
    "    channel=\"ibm_quantum_platform\",\n",
    ")\n",
    "singularity = catalog.load(FUNCTION_NAME)\n",
    "print(\n",
    "    \"Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\"\n",
    ")\n",
    "print(\"Catalog:\", catalog)\n",
    "print(\"Singularity function:\", singularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6a559-118a-4aa9-874d-9c009b5da60c",
   "metadata": {},
   "source": "### Connect to IBM Quantum and load the Singularity function\nSusunod, mag-authenticate tayo sa IBM Quantum services at ika-karga ang Singularity Machine Learning – Classification function mula sa Qiskit Functions Catalog.\nAng `QiskitRuntimeService` ay nagtatatag ng secure na koneksyon sa IBM Quantum Platform gamit ang inyong API token at instance CRN, na nagbibigay ng access sa quantum backend.\nAng `QiskitFunctionsCatalog` ay ginagamit pagkatapos upang kunin ang Singularity function sa pamamagitan ng pangalan (`\"multiverse/singularity\"`), na nagbibigay-daan sa atin na tawagin ito mamaya para sa hybrid quantum–classical computation.\nKung matagumpay ang setup, makikita ninyo ang mensahe ng kumpirmasyon na nagsasabing ang function ay na-load nang tama."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bc841e-7365-4508-b6bf-ae57db6050e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load data from the given path to X and y arrays.\"\"\"\n",
    "    df: pd.DataFrame = pd.read_csv(data_path)\n",
    "    return df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, y_true):\n",
    "    \"\"\"Compute and print accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    precision = precision_score(y_true, predictions)\n",
    "    recall = recall_score(y_true, predictions)\n",
    "    f1 = f1_score(y_true, predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988ee237",
   "metadata": {},
   "source": [
    "## Step 1: Map classical inputs to a quantum problem\n",
    "\n",
    "We begin by preparing the dataset for hybrid quantum–classical experimentation. The goal of this step is to convert the raw grid-stability data into balanced training, validation, and test splits that can be used consistently by both classical and quantum workflows. Maintaining identical splits ensures that later performance comparisons are fair and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c084cde-a5cf-4661-a00c-aa243c8b0e44",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing\n",
    "\n",
    "We first load the training and test CSV files, create a validation split, and balance the dataset using random over-sampling. Balancing prevents bias toward the majority class and provides a more stable learning signal for both classical and quantum ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db0e914-a8f2-4a04-bec7-c15bac8104b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train_bal: (5104, 12)\n",
      "  y_train_bal: (5104,)\n",
      "  X_val: (850, 12)\n",
      "  y_val: (850,)\n",
      "  X_test: (750, 12)\n",
      "  y_test: (750,)\n"
     ]
    }
   ],
   "source": [
    "# Load and upload the data\n",
    "X_train, y_train = load_data(TRAIN_PATH)\n",
    "X_test, y_test = load_data(TEST_PATH)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Balance the dataset through over-sampling of the positive class\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train_bal:\", X_train_bal.shape)\n",
    "print(\"  y_train_bal:\", y_train_bal.shape)\n",
    "print(\"  X_val:\", X_val.shape)\n",
    "print(\"  y_val:\", y_val.shape)\n",
    "print(\"  X_test:\", X_test.shape)\n",
    "print(\"  y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c214fd-09a8-4fa5-ab83-b0c71ad615d4",
   "metadata": {},
   "source": "### Define helper functions\nBago patakbuhin ang mga pangunahing eksperimento, magtutukoy tayo ng ilang maliliit na utility function na nagpapasimple ng data loading at model evaluation.\n- Ang `load_data()` ay nagbabasa ng mga input CSV file sa NumPy array, na naghihiwalay ng mga feature at label para sa compatibility sa `scikit-learn` at quantum workflow.\n- Ang `evaluate_predictions()` ay kinukumputa ang mga pangunahing performance metric - accuracy, precision, recall, at F1-score - at opsyonal na nag-uulat ng runtime kung ibinigay ang timing information.\n\nAng mga helper function na ito ay nagpapasimple ng paulit-ulit na operasyon mamaya sa notebook at nagsisiguro ng pare-parehong metric reporting sa parehong klasikal at quantum classifier."
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ecd608-35b4-4e58-8a16-85ef98bf024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical AdaBoost baseline:\n",
      "Accuracy: 0.7893333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.7893333333333333\n",
      "F1: 0.8822652757078987\n"
     ]
    }
   ],
   "source": [
    "# ----- Classical baseline: AdaBoost -----\n",
    "baseline = AdaBoostClassifier(n_estimators=60, random_state=RANDOM_STATE)\n",
    "baseline.fit(X_train_bal, y_train_bal)\n",
    "baseline_pred = baseline.predict(X_test)\n",
    "print(\"Classical AdaBoost baseline:\")\n",
    "_ = evaluate_predictions(baseline_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f36e3",
   "metadata": {},
   "source": "## Step 1: Map classical inputs to a quantum problem\nMagsisimula tayo sa pamamagitan ng paghahanda ng dataset para sa hybrid quantum–classical experimentation. Ang layunin ng hakbang na ito ay i-convert ang raw grid-stability data sa balanced training, validation, at test split na maaaring gamitin nang pare-pareho ng parehong klasikal at quantum workflow. Ang pagpapanatili ng magkaparehong split ay nagsisiguro na ang mga paghahambing ng performance sa susunod ay patas at maiuulit.\n### Data loading and preprocessing\nUna, ika-karga natin ang training at test CSV file, lumikha ng validation split, at balansehin ang dataset gamit ang random over-sampling. Ang pagbabalanse ay pumipigil sa bias tungo sa majority class at nagbibigay ng mas matatag na learning signal para sa parehong klasikal at quantum ensemble model."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c0d3f9-691b-449d-83ca-e6bfce2d6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured hardware optimization profile:\n",
      "  simulator: False\n",
      "  num_solutions: 100000\n",
      "  reps: 3\n",
      "  optimization_level: 3\n",
      "  num_transpiler_runs: 30\n",
      "  classical_optimizer: COBYLA\n",
      "  classical_optimizer_options: {'maxiter': 60}\n",
      "  estimator_options: None\n",
      "  sampler_options: None\n"
     ]
    }
   ],
   "source": [
    "# QAOA / runtime configuration for best results on hardware\n",
    "optimizer_options = {\n",
    "    \"simulator\": False,  # set True to test locally without QPU\n",
    "    \"num_solutions\": 100_000,  # broaden search over candidate ensembles\n",
    "    \"reps\": 3,  # QAOA depth (circuit layers)\n",
    "    \"optimization_level\": 3,  # transpilation effort\n",
    "    \"num_transpiler_runs\": 30,  # explore multiple layouts\n",
    "    \"classical_optimizer\": \"COBYLA\",  # robust default for this landscape\n",
    "    \"classical_optimizer_options\": {\n",
    "        \"maxiter\": 60  # practical convergence budget\n",
    "    },\n",
    "    # You can pass backend-specific options; leaving None uses least-busy routing\n",
    "    \"estimator_options\": None,\n",
    "    \"sampler_options\": None,\n",
    "}\n",
    "\n",
    "print(\"Configured hardware optimization profile:\")\n",
    "for key, value in optimizer_options.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4d480b3",
   "metadata": {},
   "source": [
    "## Step 3: Execute using Qiskit primitives\n",
    "\n",
    "We now execute the full workflow using the Singularity function’s `create_fit_predict` action to train, optimize, and evaluate the `QuantumEnhancedEnsembleClassifier` end-to-end on IBM infrastructure. The function builds the ensemble, applies quantum optimization through Qiskit primitives, and returns both predictions and job metadata (including runtime and resource usage). The classical data split from Step 1 is reused for reproducibility, with validation data passed through `fit_params` so the optimization can tune hyperparameters internally while keeping the held-out test set untouched.\n",
    "\n",
    "In this step, we explore several configurations of the quantum ensemble to understand how key parameters - specifically `num_learners` and `regularization` - affect both result quality and QPU usage.\n",
    "- `num_learners` determines the ensemble width (and implicitly, the number of qubits), influencing the model’s capacity and computational cost.\n",
    "- `regularization` controls sparsity and overfitting, shaping how many learners remain active after optimization.\n",
    "\n",
    "By varying these parameters, we can see how ensemble width and regularization interact: increasing width typically improves F1 but costs more QPU time, while stronger or adaptive regularization can improve generalization at roughly the same hardware footprint. The next subsections walk through three representative configurations to illustrate these effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da41d4d-f06d-4b67-a8ee-b07fc0289558",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "This configuration uses `num_learners = 10` and `regularization = 7`.\n",
    "\n",
    "- `num_learners` controls the ensemble width — effectively the number of weak learners combined and, on quantum hardware, the **number of qubits required**. A larger value expands the combinatorial search space and can improve accuracy and recall, but also increases circuit width, compilation time, and overall QPU usage.\n",
    "- `regularization` sets the penalty strength for including additional learners. With the default \"onsite\" regularization, higher values enforce stronger sparsity (fewer learners kept), while lower values allow more complex ensembles.\n",
    "\n",
    "This setup provides a low-cost baseline, showing how a small ensemble behaves before scaling width or tuning sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e702c986-fd7f-4ea1-a93c-8cd5ef0c4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 10\n",
    "REGULARIZATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c261c1-a3c1-42c9-a522-63f3fc01a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "Metadata: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 267.05158376693726}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 3336.8785166740417}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 152.4274561405182}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1550.1889700889587}}}\n",
      "Accuracy: 0.868\n",
      "Precision: 1.0\n",
      "Recall: 0.868\n",
      "F1: 0.9293361884368309\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_1 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_1 = job_1.result()\n",
    "print(\"Action status:\", result_1.get(\"status\"))\n",
    "print(\"Action message:\", result_1.get(\"message\"))\n",
    "print(\"Metadata:\", result_1.get(\"metadata\"))\n",
    "qeec_pred_job_1 = np.array(result_1[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf0a3ae-b5b8-43d7-b72d-5a9cf6c61060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_1 = job_1.status()\n",
    "print(\"\\nQuantum job status:\", status_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62b414-db77-45aa-bf2c-a8537fd7eba0",
   "metadata": {},
   "source": "## Step 2: Optimize problem for quantum hardware execution\nAng gawain ng ensemble selection ay ginagawang kombinatoryal na optimization problem kung saan ang bawat mahinang learner ay isang binary decision variable, at ang layunin ay nagbabalanse ng accuracy sa sparsity sa pamamagitan ng regularization term. Ang `QuantumEnhancedEnsembleClassifier` ay nalulutas ito gamit ang QAOA sa IBM hardware, habang pinapayagan pa rin ang simulator-based exploration. Ang `optimizer_options` ay kumokontrol sa hybrid loop: ang `simulator=False` ay nagdidirekta ng mga circuit sa napiling QPU, ang `num_solutions` ay nagpapataas ng search breadth, at ang `classical_optimizer_options` (para sa panloob na klasikal na optimizer) ay namamahala sa convergence; ang mga value na malapit sa 60 iteration ay magandang balanse para sa kalidad at runtime. Ang mga runtime option - tulad ng katamtamang circuit depth (`reps`) at karaniwang transpilation effort - ay tumutulong na masiguro ang matatag na performance sa iba't ibang device. Ang configuration sa ibaba ay ang \"best-results\" profile na gagamitin natin para sa hardware run; maaari din kayong lumikha ng purong simulated variant sa pamamagitan ng pagpalit sa `simulator=True` upang mag-dry-run ng workflow nang hindi gumagamit ng QPU time."
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fe64b0-2713-4b65-b768-10fa5d8dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 30\n",
    "REGULARIZATION = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a22fed-41c2-4407-88d4-02fbd32f3320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "QPU Time: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 680.2116754055023}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 80.80395102500916}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 154.4466371536255}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1095.822762966156}}}\n",
      "Accuracy: 0.8946666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.8946666666666667\n",
      "F1: 0.944405348346235\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_2 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_2 = job_2.result()\n",
    "print(\"Action status:\", result_2.get(\"status\"))\n",
    "print(\"Action message:\", result_2.get(\"message\"))\n",
    "print(\"QPU Time:\", result_2.get(\"metadata\"))\n",
    "qeec_pred_job_2 = np.array(result_2[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edd22b98-7ae6-4444-8fe5-7279e9233c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_2 = job_2.status()\n",
    "print(\"\\nQuantum job status:\", status_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94784a-3599-4911-ade9-4792b50c31bb",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "In this configuration, we increase to `num_learners = 60` and introduce adaptive regularization to manage sparsity more intuitively.\n",
    "\n",
    "- With `regularization = \"auto\"`, the optimizer automatically finds a suitable regularization strength that selects approximately `regularization_ratio * num_learners` weak learners for the final ensemble, rather than fixing the penalty manually. This provides a more convenient interface for managing the balance between sparsity and ensemble size.\n",
    "- `regularization_type = \"alpha\"` defines how the penalty is applied. Unlike `onsite`, which is unbounded `[0, ∞]`, `alpha` is bounded between `[0, 1]`, making it easier to tune and interpret. The parameter controls the trade-off between individual and pairwise penalties, offering a smoother configuration range.\n",
    "- `regularization_desired_ratio ≈ 0.82` specifies the target proportion of learners to keep active after regularization — here, around 82% of learners are retained, trimming the weakest 18% automatically.\n",
    "\n",
    "While adaptive regularization simplifies configuration and helps maintain a balanced ensemble, it does not necessarily guarantee better or more stable performance. The actual quality depends on selecting an appropriate regularization parameter, and fine-tuning it through cross-validation can be computationally expensive. The main advantage lies in improved usability and interpretability rather than direct accuracy gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209eb51a-e44b-4f94-8975-269ec7e3d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 60\n",
    "REGULARIZATION = \"auto\"\n",
    "REGULARIZATION_TYPE = \"alpha\"\n",
    "REGULARIZATION_RATIO = 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2aa9db-4427-48de-aae0-c5c1da1cb998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n",
      "Action status: ok\n",
      "Action message: Classifier created, fitted, and predicted.\n",
      "Metadata: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 1387.7451872825623}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 95.41597843170166}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 171.78878355026245}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1146.5584812164307}}}\n",
      "Accuracy: 0.908\n",
      "Precision: 1.0\n",
      "Recall: 0.908\n",
      "F1: 0.9517819706498952\n"
     ]
    }
   ],
   "source": [
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "job_3 = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    regularization_type=REGULARIZATION_TYPE,\n",
    "    regularization_desired_ratio=REGULARIZATION_RATIO,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "result_3 = job_3.result()\n",
    "print(\"Action status:\", result_3.get(\"status\"))\n",
    "print(\"Action message:\", result_3.get(\"message\"))\n",
    "print(\"Metadata:\", result_3.get(\"metadata\"))\n",
    "qeec_pred_job_3 = np.array(result_3[\"data\"][\"predictions\"])\n",
    "_ = evaluate_predictions(qeec_pred_job_3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "299b36cb-0ed8-4af3-b431-a87daec04c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantum job status: DONE\n"
     ]
    }
   ],
   "source": [
    "status_3 = job_3.status()\n",
    "print(\"\\nQuantum job status:\", status_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b94af2",
   "metadata": {},
   "source": [
    "## Step 4: Post-process and return result in desired classical format\n",
    "\n",
    "We now post-process outputs from both the classical and quantum runs, converting them into a consistent format for downstream evaluation. This step compares predictive quality using standard metrics - accuracy, precision, recall, and F1 - and analyzes how ensemble width (`num_learners`) and sparsity control (`regularization`) influence both performance and computational behavior.\n",
    "\n",
    "The classical AdaBoost baseline provides a compact and stable reference for small-scale learning. It performs well with limited ensembles and negligible compute overhead, reflecting the strength of traditional boosting when the hypothesis space is still tractable. The quantum configurations (`qeec_pred_job_1`, `qeec_pred_job_2`, and `qeec_pred_job_3`) extend this baseline by embedding the ensemble-selection process within a variational quantum optimization loop. This allows the system to explore exponentially large subsets of learners simultaneously in superposition, addressing the combinatorial nature of ensemble selection more efficiently as scale increases.\n",
    "\n",
    "Results show that increasing `num_learners` from 10 to 30 improves recall and F1, confirming that a wider ensemble captures richer interactions among weak learners. The gain is sublinear on current hardware - each additional learner yields smaller accuracy increments - but the underlying scaling behavior remains favorable because the quantum optimizer can search broader configuration spaces without the exponential blow-up typical of classical subset selection. Regularization introduces additional nuance: a fixed λ=7 enforces consistent sparsity and stabilizes convergence, whereas adaptive α-regularization automatically tunes sparsity based on correlations between learners. This dynamic pruning often achieves slightly higher F1 for the same qubit width, balancing model complexity and generalization.\n",
    "\n",
    "When compared directly with the AdaBoost baseline, the smallest quantum configuration (L=10) reproduces similar accuracy, validating the hybrid pipeline’s correctness. At larger widths, quantum variants - especially with auto-regularization - begin to surpass the classical baseline modestly, showing improved recall and F1 without linear growth in computational cost. These improvements do not indicate immediate \"quantum advantage\" but rather **scaling efficiency**: the quantum optimizer maintains tractable performance as the ensemble expands, where a classical approach would face exponential growth in subset-selection complexity.\n",
    "\n",
    "In practice:\n",
    "- Use the **classical baseline** for quick validation and benchmarking on small datasets.\n",
    "- Apply **quantum ensembles** when model width or feature complexity grows—QAOA-based search scales more gracefully in those regimes.\n",
    "- Employ **adaptive α-regularization** to maintain sparsity and generalization without increasing circuit width.\n",
    "- Monitor QPU time and depth to balance quality gains against near-term hardware constraints.\n",
    "\n",
    "Together, these experiments show that quantum-optimized ensembles complement classical methods: they reproduce baseline accuracy at small scales while offering a path to efficient scaling on larger, combinatorial learning problems. As hardware improves, these scaling advantages are expected to compound, extending the feasible size and depth of ensemble-based models beyond what is classically practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1d610-f2e2-482c-af05-a882bb380528",
   "metadata": {},
   "source": [
    "### Evaluate metrics for each configuration\n",
    "\n",
    "We now evaluate all configurations - the classical AdaBoost baseline and the three quantum ensembles - using the `evaluate_predictions` helper to compute accuracy, precision, recall, and F1 on the same test set. This comparison clarifies how quantum optimization scales relative to the classical approach: at small widths, both perform similarly; as ensembles grow, the quantum method can explore larger hypothesis spaces more efficiently. The resulting table captures these trends in a consistent, quantitative form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8cc821-8311-4c84-8f94-ffb2ec3facc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7893333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.7893333333333333\n",
      "F1: 0.8822652757078987\n",
      "Accuracy: 0.868\n",
      "Precision: 1.0\n",
      "Recall: 0.868\n",
      "F1: 0.9293361884368309\n",
      "Accuracy: 0.8946666666666667\n",
      "Precision: 1.0\n",
      "Recall: 0.8946666666666667\n",
      "F1: 0.944405348346235\n",
      "Accuracy: 0.908\n",
      "Precision: 1.0\n",
      "Recall: 0.908\n",
      "F1: 0.9517819706498952\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Config</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost (Classical)</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>0.882265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QEEC L=10, reg=7</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.929336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QEEC L=30, reg=7</td>\n",
       "      <td>0.894667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.894667</td>\n",
       "      <td>0.944405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QEEC L=60, reg=auto (α=0.82)</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.951782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Config  Accuracy  Precision    Recall        F1\n",
       "0          AdaBoost (Classical)  0.789333        1.0  0.789333  0.882265\n",
       "1              QEEC L=10, reg=7  0.868000        1.0  0.868000  0.929336\n",
       "2              QEEC L=30, reg=7  0.894667        1.0  0.894667  0.944405\n",
       "3  QEEC L=60, reg=auto (α=0.82)  0.908000        1.0  0.908000  0.951782"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Classical baseline\n",
    "acc_b, prec_b, rec_b, f1_b = evaluate_predictions(baseline_pred, y_test)\n",
    "results.append(\n",
    "    {\n",
    "        \"Config\": \"AdaBoost (Classical)\",\n",
    "        \"Accuracy\": acc_b,\n",
    "        \"Precision\": prec_b,\n",
    "        \"Recall\": rec_b,\n",
    "        \"F1\": f1_b,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Quantum runs\n",
    "for label, preds in [\n",
    "    (\"QEEC L=10, reg=7\", qeec_pred_job_1),\n",
    "    (\"QEEC L=30, reg=7\", qeec_pred_job_2),\n",
    "    (f\"QEEC L=60, reg=auto (α={REGULARIZATION_RATIO})\", qeec_pred_job_3),\n",
    "]:\n",
    "    acc, prec, rec, f1 = evaluate_predictions(preds, y_test)\n",
    "    results.append(\n",
    "        {\n",
    "            \"Config\": label,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cecf6-2c87-4356-b9be-26f4346c194f",
   "metadata": {},
   "source": "### Increase the number of learners\nDito ay dinadagdagan natin ang `num_learners` mula 10 → 30 habang pinapanatili ang `regularization = 7`.\n\n- Ang mas maraming learner ay nagpapalawig ng hypothesis space, na nagpapahintulot sa modelo na makuha ang mas banayad na pattern, na maaaring bahagyang magtaas ng F1.\n- Sa karamihan ng mga kaso, ang pagkakaiba ng runtime sa pagitan ng 10 at 30 learner ay hindi gaanong malaki, na nagsasaad na ang karagdagang circuit width ay hindi lubhang nagpapataas ng execution cost.\n- Ang pagpapabuti ng kalidad ay sumusunod pa rin sa *diminishing-returns curve*: ang mga unang pakinabang ay lumalabas habang lumalaki ang ensemble, ngunit nag-plateau ang mga ito habang ang karagdagang mga learner ay nag-aambag ng mas kaunting bagong impormasyon.\n\nAng eksperimentong ito ay nag-highlight ng quality–efficiency trade-off — ang pagtaas ng ensemble width ay maaaring mag-alok ng maliliit na accuracy gain nang walang malaking runtime penalty, depende sa backend at transpilation condition."
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f15c5fb-2450-4671-9bc2-471043414df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image src=\"../docs/images/tutorials/sml-classification/extracted-outputs/0f15c5fb-2450-4671-9bc2-471043414df2-0.avif\" alt=\"Output of the previous code cell\" />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(7.6, 4.6))\n",
    "plt.bar(x - width / 2, df_results[\"Accuracy\"], width=width, label=\"Accuracy\")\n",
    "plt.bar(x + width / 2, df_results[\"F1\"], width=width, label=\"F1\")\n",
    "plt.xticks(x, df_results[\"Config\"], rotation=10)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Classical vs Quantum ensemble performance\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cd406-5166-4eb9-8506-0eedd71e9b79",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The plot confirms the expected scaling pattern. The classical AdaBoost performs strongly for smaller ensembles but becomes increasingly costly to scale as the number of weak learners grows, because its subset-selection problem expands combinatorially. The quantum-enhanced models replicate classical accuracy at low widths and begin to surpass it as ensemble size increases, especially under adaptive α-regularization. This reflects the quantum optimizer’s ability to sample and evaluate many candidate subsets in parallel through superposition, maintaining tractable search even at higher widths. While current hardware overhead offsets some of the theoretical gains, the trend illustrates the scaling efficiency advantage of the quantum formulation. In practical terms, the classical method remains preferable for lightweight benchmarks, while quantum-enhanced ensembles become advantageous as model dimensionality and ensemble size expand, offering better trade-offs between accuracy, generalization, and computational growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27afb68-f959-4f4a-910c-0aadafb7e18e",
   "metadata": {},
   "source": [
    "## Appendix: Scaling benefits and enhancements\n",
    "\n",
    "The scalability advantage of the `QuantumEnhancedEnsembleClassifier` arises from how the ensemble-selection process maps to quantum optimization.\n",
    "Classical ensemble learning methods, such as AdaBoost or random forests, become computationally expensive as the number of weak learners increases because selecting the optimal subset is a combinatorial problem that scales exponentially.\n",
    "\n",
    "In contrast, the quantum formulation — implemented here via the Quantum Approximate Optimization Algorithm (QAOA) — can explore these exponentially large search spaces more efficiently by evaluating multiple configurations in superposition.\n",
    "As a result, the training time does not grow significantly with the number of learners, allowing the model to remain efficient even as ensemble width increases.\n",
    "\n",
    "While current hardware introduces some noise and depth limitations, this workflow demonstrates a near-term hybrid approach where classical and quantum components cooperate: the quantum optimizer provides a better initialization landscape for the classical loop, improving convergence and final model quality.\n",
    "As quantum processors evolve, these scalability benefits are expected to extend to larger datasets, broader ensembles, and deeper circuit depths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41a301",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Introduction to Qiskit Functions](/docs/guides/functions)\n",
    "2. [Multiverse Computing Singularity Machine Learning](/docs/guides/multiverse-computing-singularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5785c",
   "metadata": {},
   "source": [
    "## Tutorial survey\n",
    "\n",
    "Please take a minute to provide feedback on this tutorial. Your insights will help us improve our content offerings and user experience.\n",
    "\n",
    "[Link to survey](https://your.feedback.ibm.com/jfe/form/SV_3BLFkNVEuh0QBWm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  },
  "colab": {
   "cell_execution_strategy": "setup"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}