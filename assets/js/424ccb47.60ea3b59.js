"use strict";(globalThis.webpackChunkdoqumentation=globalThis.webpackChunkdoqumentation||[]).push([[9778],{15176(e,n,a){a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"tutorials/sml-classification","title":"Hybrid quantum-enhanced ensemble classification (grid stability workflow)","description":"Build and analyze a hybrid quantum\u2013classical ensemble for grid-stability classification on IBM QPUs using Multiverse Computing\'s Singularity Qiskit Function.","source":"@site/i18n/tl/docusaurus-plugin-content-docs/current/tutorials/sml-classification.mdx","sourceDirName":"tutorials","slug":"/tutorials/sml-classification","permalink":"/tutorials/sml-classification","draft":false,"unlisted":false,"editUrl":"https://github.com/JanLahmann/doQumentation/tree/main/docs/tutorials/sml-classification.mdx","tags":[],"version":"current","frontMatter":{"title":"Hybrid quantum-enhanced ensemble classification (grid stability workflow)","sidebar_label":"Hybrid quantum-enhanced ensemble classification (grid stability workflow)","description":"Build and analyze a hybrid quantum\u2013classical ensemble for grid-stability classification on IBM QPUs using Multiverse Computing\'s Singularity Qiskit Function.","notebook_path":"docs/tutorials/sml-classification.ipynb"},"sidebar":"tutorialsSidebar","previous":{"title":"Solve the Market Split problem with Kipu Quantum\'s Iskay Quantum Optimizer","permalink":"/tutorials/solve-market-split-problem-with-iskay-quantum-optimizer"},"next":{"title":"Simulate 2D tilted-field Ising with the QESEM function","permalink":"/tutorials/qedma-2d-ising-with-qesem"}}');var t=a(74848),s=a(28453);const r={title:"Hybrid quantum-enhanced ensemble classification (grid stability workflow)",sidebar_label:"Hybrid quantum-enhanced ensemble classification (grid stability workflow)",description:"Build and analyze a hybrid quantum\u2013classical ensemble for grid-stability classification on IBM QPUs using Multiverse Computing's Singularity Qiskit Function.",notebook_path:"docs/tutorials/sml-classification.ipynb"},l=void 0,o={},c=[{value:"Background",id:"background",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Setup",id:"setup",level:2},{value:"Download the dataset",id:"download-the-dataset",level:3},{value:"Import required packages",id:"import-required-packages",level:3},{value:"Set constant variables",id:"set-constant-variables",level:3},{value:"Connect to IBM Quantum and load the Singularity function",id:"connect-to-ibm-quantum-and-load-the-singularity-function",level:3},{value:"Define helper functions",id:"define-helper-functions",level:3},{value:"Step 1: Map classical inputs to a quantum problem",id:"step-1-map-classical-inputs-to-a-quantum-problem",level:2},{value:"Data loading and preprocessing",id:"data-loading-and-preprocessing",level:3},{value:"Classical baseline: AdaBoost reference",id:"classical-baseline-adaboost-reference",level:3},{value:"Step 2: Optimize problem for quantum hardware execution",id:"step-2-optimize-problem-for-quantum-hardware-execution",level:2},{value:"Step 3: Execute using Qiskit primitives",id:"step-3-execute-using-qiskit-primitives",level:2},{value:"Baseline",id:"baseline",level:3},{value:"Increase the number of learners",id:"increase-the-number-of-learners",level:3},{value:"Regularization",id:"regularization",level:3},{value:"Step 4: Post-process and return result in desired classical format",id:"step-4-post-process-and-return-result-in-desired-classical-format",level:2},{value:"Evaluate metrics for each configuration",id:"evaluate-metrics-for-each-configuration",level:3},{value:"Visualize quality trends across configurations",id:"visualize-quality-trends-across-configurations",level:3},{value:"Interpretation",id:"interpretation",level:3},{value:"Appendix: Scaling benefits and enhancements",id:"appendix-scaling-benefits-and-enhancements",level:2},{value:"References",id:"references",level:2},{value:"Tutorial survey",id:"tutorial-survey",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{OpenInLabBanner:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("OpenInLabBanner",!0),(0,t.jsxs)(t.Fragment,{children:["\n",(0,t.jsx)(n.admonition,{title:"Hindi pa naisalin",type:"note",children:(0,t.jsx)(n.p,{children:"Ang pahinang ito ay hindi pa naisalin. Nakikita mo ang orihinal na bersyon sa Ingles."})}),"\n",(0,t.jsx)(i,{notebookPath:"docs/tutorials/sml-classification.ipynb"}),"\n","\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Usage estimate: 20 minutes in QPU time for each job on an Eagle r3 processor. (NOTE: This is an estimate only. Your runtime may vary.)"})}),"\n",(0,t.jsx)(n.h2,{id:"background",children:"Background"}),"\n",(0,t.jsx)(n.p,{children:"This tutorial demonstrates a hybrid quantum\u2013classical workflow that enhances a classical ensemble with a quantum optimization step. Using Multiverse Computing\u2019s \u201cSingularity Machine Learning \u2013 Classification\u201d (a Qiskit Function), we train a pool of conventional learners (for example, decision trees, k-NN, logistic regression) and then refine that pool with a quantum layer to improve diversity and generalization. The objective is practical: on a real grid-stability prediction task, we compare a strong classical baseline with a quantum-optimized alternative under the same data splits, so you can see where the quantum step helps and what it costs."}),"\n",(0,t.jsx)(n.p,{children:"Why this matters: selecting a good subset from many weak learners is a combinatorial problem that grows quickly with ensemble size. Classical heuristics like boosting, bagging, and stacking perform well at moderate scales but can struggle to explore large, redundant libraries of models efficiently. The function integrates quantum algorithms - specifically QAOA (and optionally VQE in other configurations) - to search that space more effectively after the classical learners are trained, increasing the chance of finding a compact, diverse subset that generalizes better."}),"\n",(0,t.jsx)(n.p,{children:"Crucially, data scale is not limited by qubits. The heavy lifting on data \u2014 preprocessing, training the learner pool, and evaluation \u2014 remains classical and can handle millions of examples. Qubits only determine the ensemble size used in the quantum selection step. This decoupling is what makes the approach viable on today\u2019s hardware: you keep familiar scikit-learn workflows for data and model training while calling the quantum step through a clean action interface in Qiskit Functions."}),"\n",(0,t.jsx)(n.p,{children:"In practice, while different learner types can be provided to the ensemble (e.g., decision trees, logistic regression, or k-NN), Decision Trees tend to perform best. The optimizer consistently favors stronger ensemble members\u2014when heterogeneous learners are supplied, weaker models such as linear regressors are typically pruned in favor of more expressive ones like Decision Trees."}),"\n",(0,t.jsxs)(n.p,{children:["What you will do here: prepare and balance the grid-stability dataset; establish a classical AdaBoost baseline; run several quantum configurations that vary ensemble width and regularization; execute on IBM\xae simulators or QPUs via Qiskit Serverless; and compare accuracy, precision, recall, and F1 across all runs. Along the way, you will use the function\u2019s action pattern (",(0,t.jsx)(n.code,{children:"create"}),", ",(0,t.jsx)(n.code,{children:"fit"}),", ",(0,t.jsx)(n.code,{children:"predict"}),", ",(0,t.jsx)(n.code,{children:"fit_predict"}),", ",(0,t.jsx)(n.code,{children:"create_fit_predict"}),") and key controls:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Regularization types: ",(0,t.jsx)(n.code,{children:"onsite"})," (\u03bb) for direct sparsity and ",(0,t.jsx)(n.code,{children:"alpha"})," for a ratio-based trade-off between interaction and onsite terms"]}),"\n",(0,t.jsxs)(n.li,{children:["Auto-regularization: set ",(0,t.jsx)(n.code,{children:'regularization="auto"'})," with a target selection ratio to adapt sparsity automatically"]}),"\n",(0,t.jsx)(n.li,{children:"Optimizer options: simulator versus QPU, repetitions, classical optimizer and its options, transpilation depth, and runtime sampler/estimator settings"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Benchmarks in the documentation show that accuracy improves as the number of learners (qubits) increases on challenging problems, with the quantum classifier matching or exceeding a comparable classical ensemble. In this tutorial, you will reproduce the workflow end-to-end and examine when increasing ensemble width or switching to adaptive regularization yields better F1 at reasonable resource usage. The result is a grounded view of how a quantum optimization step can complement, rather than replace, classical ensemble learning in real applications."}),"\n",(0,t.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,t.jsx)(n.p,{children:"Before starting this tutorial, ensure you have the following packages installed in your Python environment:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"qiskit[visualization]~=2.1.0"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"qiskit-serverless~=0.24.0"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"qiskit-ibm-runtime v0.40.1"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"qiskit-ibm-catalog~=0.8.0"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"scikit-learn==1.5.2"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"pandas>=2.0.0,<3.0.0"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"imbalanced-learn~=0.12.3"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsx)(n.p,{children:"In this section, we initialize the Qiskit Serverless client and load the Singularity Machine Learning \u2013 Classification function provided by Multiverse Computing.\nWith Qiskit Serverless, you can run hybrid quantum\u2013classical workflows on IBM managed cloud infrastructure without worrying about resource management.\nYou will need an IBM Quantum Platform API key and your cloud resource name (CRN) to authenticate and access Qiskit Functions."}),"\n",(0,t.jsx)(n.h3,{id:"download-the-dataset",children:"Download the dataset"}),"\n",(0,t.jsxs)(n.p,{children:["To run this tutorial, we use a preprocessed ",(0,t.jsx)(n.strong,{children:"grid stability classification dataset"})," containing labeled power system sensor readings.\nThe following cell automatically creates the required folder structure and downloads both the training and test files directly into your environment using ",(0,t.jsx)(n.code,{children:"wget"}),".\nIf you already have these files locally, this step will safely overwrite them to ensure version consistency."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Added by doQumentation \u2014 installs packages not in the Binder environment\n%pip install -q imbalanced-learn scikit-learn\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'## Download dataset for Grid Stability Classification\n\n# Create data directory if it doesn\'t exist\n!mkdir -p data_tutorial/grid_stability\n\n# Download the training and test sets from the official Qiskit documentation repo\n!wget -q --show-progress -O data_tutorial/grid_stability/train.csv \\\n  https://raw.githubusercontent.com/Qiskit/documentation/main/datasets/tutorials/grid_stability/train.csv\n\n!wget -q --show-progress -O data_tutorial/grid_stability/test.csv \\\n  https://raw.githubusercontent.com/Qiskit/documentation/main/datasets/tutorials/grid_stability/test.csv\n\n# Check the files have been downloaded\n!echo "Dataset files downloaded:"\n!ls -lh data_tutorial/grid_stability/*.csv\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"data_tutorial/grid_ 100%[===================>] 612.94K  --.-KB/s    in 0.01s   \ndata_tutorial/grid_ 100%[===================>] 108.19K  --.-KB/s    in 0.006s  \nDataset files downloaded:\n-rw-r--r-- 1 coder coder 109K Nov  8 18:50 data_tutorial/grid_stability/test.csv\n-rw-r--r-- 1 coder coder 613K Nov  8 18:50 data_tutorial/grid_stability/train.csv\n"})}),"\n",(0,t.jsx)(n.h3,{id:"import-required-packages",children:"Import required packages"}),"\n",(0,t.jsxs)(n.p,{children:["In this section, we import all Python packages and Qiskit modules used throughout the tutorial.\nThese include core scientific libraries for data handling and model evaluation - such as ",(0,t.jsx)(n.code,{children:"NumPy"}),", ",(0,t.jsx)(n.code,{children:"pandas"}),", and ",(0,t.jsx)(n.code,{children:"scikit-learn"})," - along with visualization tools and Qiskit components for running the quantum-enhanced model.\nWe also import the ",(0,t.jsx)(n.code,{children:"QiskitRuntimeService"})," and ",(0,t.jsx)(n.code,{children:"QiskitFunctionsCatalog"})," to connect with IBM Quantum\xae services and access the Singularity Machine Learning function."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from typing import Tuple\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom imblearn.over_sampling import RandomOverSampler\nfrom qiskit_ibm_catalog import QiskitFunctionsCatalog\nfrom qiskit_ibm_runtime import QiskitRuntimeService\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    precision_score,\n    recall_score,\n)\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings("ignore")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"set-constant-variables",children:"Set constant variables"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'IBM_TOKEN = ""\nIBM_INSTANCE_TEST = ""\nIBM_INSTANCE_QUANTUM = ""\nFUNCTION_NAME = "multiverse/singularity"\nRANDOM_STATE: int = 123\nTRAIN_PATH = "data_tutorial/grid_stability/train.csv"\nTEST_PATH = "data_tutorial/grid_stability/test.csv"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"connect-to-ibm-quantum-and-load-the-singularity-function",children:"Connect to IBM Quantum and load the Singularity function"}),"\n",(0,t.jsxs)(n.p,{children:["Next, we authenticate with IBM Quantum services and load the Singularity Machine Learning \u2013 Classification function from the Qiskit Functions Catalog.\nThe ",(0,t.jsx)(n.code,{children:"QiskitRuntimeService"})," establishes a secure connection to IBM Quantum Platform using your API token and instance CRN, allowing access to quantum backends.\nThe ",(0,t.jsx)(n.code,{children:"QiskitFunctionsCatalog"})," is then used to retrieve the Singularity function by name (",(0,t.jsx)(n.code,{children:'"multiverse/singularity"'}),"), enabling us to call it later for hybrid quantum\u2013classical computation.\nIf the setup is successful, you will see a confirmation message indicating that the function has been loaded correctly."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'service = QiskitRuntimeService(\n    token=IBM_TOKEN,\n    channel="ibm_quantum_platform",\n    instance=IBM_INSTANCE_QUANTUM,\n)\n\nbackend = service.least_busy()\ncatalog = QiskitFunctionsCatalog(\n    token=IBM_TOKEN,\n    instance=IBM_INSTANCE_TEST,\n    channel="ibm_quantum_platform",\n)\nsingularity = catalog.load(FUNCTION_NAME)\nprint(\n    "Successfully connected to IBM Qiskit Serverless and loaded the Singularity function."\n)\nprint("Catalog:", catalog)\nprint("Singularity function:", singularity)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\nCatalog: <QiskitFunctionsCatalog>\nSingularity function: QiskitFunction(multiverse/singularity)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"define-helper-functions",children:"Define helper functions"}),"\n",(0,t.jsx)(n.p,{children:"Before running the main experiments, we define a few small utility functions that streamline data loading and model evaluation."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"load_data()"})," reads the input CSV files into NumPy arrays, splitting features and labels for compatibility with ",(0,t.jsx)(n.code,{children:"scikit-learn"})," and quantum workflows."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"evaluate_predictions()"})," computes key performance metrics - accuracy, precision, recall, and F1-score - and optionally reports runtime if timing information is provided."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These helper functions simplify repeated operations later in the notebook and ensure consistent metric reporting across both classical and quantum classifiers."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def load_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n    """Load data from the given path to X and y arrays."""\n    df: pd.DataFrame = pd.read_csv(data_path)\n    return df.iloc[:, :-1].values, df.iloc[:, -1].values\n\ndef evaluate_predictions(predictions, y_true):\n    """Compute and print accuracy, precision, recall, and F1 score."""\n    accuracy = accuracy_score(y_true, predictions)\n    precision = precision_score(y_true, predictions)\n    recall = recall_score(y_true, predictions)\n    f1 = f1_score(y_true, predictions)\n    print("Accuracy:", accuracy)\n    print("Precision:", precision)\n    print("Recall:", recall)\n    print("F1:", f1)\n    return accuracy, precision, recall, f1\n'})}),"\n",(0,t.jsx)(n.h2,{id:"step-1-map-classical-inputs-to-a-quantum-problem",children:"Step 1: Map classical inputs to a quantum problem"}),"\n",(0,t.jsx)(n.p,{children:"We begin by preparing the dataset for hybrid quantum\u2013classical experimentation. The goal of this step is to convert the raw grid-stability data into balanced training, validation, and test splits that can be used consistently by both classical and quantum workflows. Maintaining identical splits ensures that later performance comparisons are fair and reproducible."}),"\n",(0,t.jsx)(n.h3,{id:"data-loading-and-preprocessing",children:"Data loading and preprocessing"}),"\n",(0,t.jsx)(n.p,{children:"We first load the training and test CSV files, create a validation split, and balance the dataset using random over-sampling. Balancing prevents bias toward the majority class and provides a more stable learning signal for both classical and quantum ensemble models."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Load and upload the data\nX_train, y_train = load_data(TRAIN_PATH)\nX_test, y_test = load_data(TEST_PATH)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n)\n\n# Balance the dataset through over-sampling of the positive class\nros = RandomOverSampler(random_state=RANDOM_STATE)\nX_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n\nprint("Shapes:")\nprint("  X_train_bal:", X_train_bal.shape)\nprint("  y_train_bal:", y_train_bal.shape)\nprint("  X_val:", X_val.shape)\nprint("  y_val:", y_val.shape)\nprint("  X_test:", X_test.shape)\nprint("  y_test:", y_test.shape)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Shapes:\n  X_train_bal: (5104, 12)\n  y_train_bal: (5104,)\n  X_val: (850, 12)\n  y_val: (850,)\n  X_test: (750, 12)\n  y_test: (750,)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"classical-baseline-adaboost-reference",children:"Classical baseline: AdaBoost reference"}),"\n",(0,t.jsx)(n.p,{children:"Before running any quantum optimization, we train a strong classical baseline - a standard AdaBoost classifier - on the same balanced data. This provides a reproducible reference point for later comparison, helping to quantify whether quantum optimization improves generalization or efficiency beyond a well-tuned classical ensemble."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ----- Classical baseline: AdaBoost -----\nbaseline = AdaBoostClassifier(n_estimators=60, random_state=RANDOM_STATE)\nbaseline.fit(X_train_bal, y_train_bal)\nbaseline_pred = baseline.predict(X_test)\nprint("Classical AdaBoost baseline:")\n_ = evaluate_predictions(baseline_pred, y_test)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Classical AdaBoost baseline:\nAccuracy: 0.7893333333333333\nPrecision: 1.0\nRecall: 0.7893333333333333\nF1: 0.8822652757078987\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-2-optimize-problem-for-quantum-hardware-execution",children:"Step 2: Optimize problem for quantum hardware execution"}),"\n",(0,t.jsxs)(n.p,{children:["The ensemble selection task is cast as a combinatorial optimization problem where each weak learner is a binary decision variable, and the objective balances accuracy with sparsity through a regularization term. The ",(0,t.jsx)(n.code,{children:"QuantumEnhancedEnsembleClassifier"})," solves this with QAOA on IBM hardware, while still allowing simulator-based exploration. The ",(0,t.jsx)(n.code,{children:"optimizer_options"})," control the hybrid loop: ",(0,t.jsx)(n.code,{children:"simulator=False"})," routes circuits to the selected QPU, ",(0,t.jsx)(n.code,{children:"num_solutions"})," increases search breadth, and ",(0,t.jsx)(n.code,{children:"classical_optimizer_options"})," (for the inner classical optimizer) govern convergence; values around 60 iterations are a good balance for quality and runtime. Runtime options - such as moderate circuit depth (",(0,t.jsx)(n.code,{children:"reps"}),") and a standard transpilation effort - help ensure robust performance across devices. The configuration below is the \u201cbest-results\u201d profile we will use for hardware runs; you may also create a purely simulated variant by toggling ",(0,t.jsx)(n.code,{children:"simulator=True"})," to dry-run the workflow without consuming QPU time."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# QAOA / runtime configuration for best results on hardware\noptimizer_options = {\n    "simulator": False,  # set True to test locally without QPU\n    "num_solutions": 100_000,  # broaden search over candidate ensembles\n    "reps": 3,  # QAOA depth (circuit layers)\n    "optimization_level": 3,  # transpilation effort\n    "num_transpiler_runs": 30,  # explore multiple layouts\n    "classical_optimizer": "COBYLA",  # robust default for this landscape\n    "classical_optimizer_options": {\n        "maxiter": 60  # practical convergence budget\n    },\n    # You can pass backend-specific options; leaving None uses least-busy routing\n    "estimator_options": None,\n    "sampler_options": None,\n}\n\nprint("Configured hardware optimization profile:")\nfor key, value in optimizer_options.items():\n    print(f"  {key}: {value}")\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Configured hardware optimization profile:\n  simulator: False\n  num_solutions: 100000\n  reps: 3\n  optimization_level: 3\n  num_transpiler_runs: 30\n  classical_optimizer: COBYLA\n  classical_optimizer_options: {'maxiter': 60}\n  estimator_options: None\n  sampler_options: None\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-3-execute-using-qiskit-primitives",children:"Step 3: Execute using Qiskit primitives"}),"\n",(0,t.jsxs)(n.p,{children:["We now execute the full workflow using the Singularity function\u2019s ",(0,t.jsx)(n.code,{children:"create_fit_predict"})," action to train, optimize, and evaluate the ",(0,t.jsx)(n.code,{children:"QuantumEnhancedEnsembleClassifier"})," end-to-end on IBM infrastructure. The function builds the ensemble, applies quantum optimization through Qiskit primitives, and returns both predictions and job metadata (including runtime and resource usage). The classical data split from Step 1 is reused for reproducibility, with validation data passed through ",(0,t.jsx)(n.code,{children:"fit_params"})," so the optimization can tune hyperparameters internally while keeping the held-out test set untouched."]}),"\n",(0,t.jsxs)(n.p,{children:["In this step, we explore several configurations of the quantum ensemble to understand how key parameters - specifically ",(0,t.jsx)(n.code,{children:"num_learners"})," and ",(0,t.jsx)(n.code,{children:"regularization"})," - affect both result quality and QPU usage."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_learners"})," determines the ensemble width (and implicitly, the number of qubits), influencing the model\u2019s capacity and computational cost."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"regularization"})," controls sparsity and overfitting, shaping how many learners remain active after optimization."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By varying these parameters, we can see how ensemble width and regularization interact: increasing width typically improves F1 but costs more QPU time, while stronger or adaptive regularization can improve generalization at roughly the same hardware footprint. The next subsections walk through three representative configurations to illustrate these effects."}),"\n",(0,t.jsx)(n.h3,{id:"baseline",children:"Baseline"}),"\n",(0,t.jsxs)(n.p,{children:["This configuration uses ",(0,t.jsx)(n.code,{children:"num_learners = 10"})," and ",(0,t.jsx)(n.code,{children:"regularization = 7"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_learners"})," controls the ensemble width \u2014 effectively the number of weak learners combined and, on quantum hardware, the ",(0,t.jsx)(n.strong,{children:"number of qubits required"}),". A larger value expands the combinatorial search space and can improve accuracy and recall, but also increases circuit width, compilation time, and overall QPU usage."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"regularization"}),' sets the penalty strength for including additional learners. With the default "onsite" regularization, higher values enforce stronger sparsity (fewer learners kept), while lower values allow more complex ensembles.']}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This setup provides a low-cost baseline, showing how a small ensemble behaves before scaling width or tuning sparsity."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Problem scale and regularization\nNUM_LEARNERS = 10\nREGULARIZATION = 7\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ----- Quantum-enhanced ensemble on IBM hardware -----\nprint("\\n-- Submitting quantum-enhanced ensemble job --")\njob_1 = singularity.run(\n    action="create_fit_predict",\n    name="grid_stability_qeec",\n    quantum_classifier="QuantumEnhancedEnsembleClassifier",\n    num_learners=NUM_LEARNERS,\n    regularization=REGULARIZATION,\n    optimizer_options=optimizer_options,  # from Step 2\n    backend_name=backend,  # least-busy compatible backend\n    instance=IBM_INSTANCE_QUANTUM,\n    random_state=RANDOM_STATE,\n    X_train=X_train_bal,\n    y_train=y_train_bal,\n    X_test=X_test,\n    fit_params={"validation_data": (X_val, y_val)},\n    options={"save": False},\n)\nresult_1 = job_1.result()\nprint("Action status:", result_1.get("status"))\nprint("Action message:", result_1.get("message"))\nprint("Metadata:", result_1.get("metadata"))\nqeec_pred_job_1 = np.array(result_1["data"]["predictions"])\n_ = evaluate_predictions(qeec_pred_job_1, y_test)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"-- Submitting quantum-enhanced ensemble job --\nAction status: ok\nAction message: Classifier created, fitted, and predicted.\nMetadata: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 267.05158376693726}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 3336.8785166740417}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 152.4274561405182}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1550.1889700889587}}}\nAccuracy: 0.868\nPrecision: 1.0\nRecall: 0.868\nF1: 0.9293361884368309\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'status_1 = job_1.status()\nprint("\\nQuantum job status:", status_1)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Quantum job status: DONE\n"})}),"\n",(0,t.jsx)(n.h3,{id:"increase-the-number-of-learners",children:"Increase the number of learners"}),"\n",(0,t.jsxs)(n.p,{children:["Here we increase ",(0,t.jsx)(n.code,{children:"num_learners"})," from 10 \u2192 30 while keeping ",(0,t.jsx)(n.code,{children:"regularization = 7"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"More learners expand the hypothesis space, allowing the model to capture more subtle patterns, which can modestly lift F1."}),"\n",(0,t.jsx)(n.li,{children:"In most cases, the runtime difference between 10 and 30 learners is not substantial, indicating that the added circuit width does not significantly increase execution cost."}),"\n",(0,t.jsxs)(n.li,{children:["The improvement in quality still follows a ",(0,t.jsx)(n.em,{children:"diminishing-returns curve"}),": early gains appear as the ensemble grows, but they plateau as additional learners contribute less new information."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This experiment highlights the quality\u2013efficiency trade-off \u2014 increasing ensemble width may offer small accuracy gains without a major runtime penalty, depending on backend and transpilation conditions."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Problem scale and regularization\nNUM_LEARNERS = 30\nREGULARIZATION = 7\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ----- Quantum-enhanced ensemble on IBM hardware -----\nprint("\\n-- Submitting quantum-enhanced ensemble job --")\njob_2 = singularity.run(\n    action="create_fit_predict",\n    name="grid_stability_qeec",\n    quantum_classifier="QuantumEnhancedEnsembleClassifier",\n    num_learners=NUM_LEARNERS,\n    regularization=REGULARIZATION,\n    optimizer_options=optimizer_options,  # from Step 2\n    backend_name=backend,  # least-busy compatible backend\n    instance=IBM_INSTANCE_QUANTUM,\n    random_state=RANDOM_STATE,\n    X_train=X_train_bal,\n    y_train=y_train_bal,\n    X_test=X_test,\n    fit_params={"validation_data": (X_val, y_val)},\n    options={"save": False},\n)\nresult_2 = job_2.result()\nprint("Action status:", result_2.get("status"))\nprint("Action message:", result_2.get("message"))\nprint("QPU Time:", result_2.get("metadata"))\nqeec_pred_job_2 = np.array(result_2["data"]["predictions"])\n_ = evaluate_predictions(qeec_pred_job_2, y_test)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"-- Submitting quantum-enhanced ensemble job --\nAction status: ok\nAction message: Classifier created, fitted, and predicted.\nQPU Time: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 680.2116754055023}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 80.80395102500916}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 154.4466371536255}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1095.822762966156}}}\nAccuracy: 0.8946666666666667\nPrecision: 1.0\nRecall: 0.8946666666666667\nF1: 0.944405348346235\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'status_2 = job_2.status()\nprint("\\nQuantum job status:", status_2)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Quantum job status: DONE\n"})}),"\n",(0,t.jsx)(n.h3,{id:"regularization",children:"Regularization"}),"\n",(0,t.jsxs)(n.p,{children:["In this configuration, we increase to ",(0,t.jsx)(n.code,{children:"num_learners = 60"})," and introduce adaptive regularization to manage sparsity more intuitively."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["With ",(0,t.jsx)(n.code,{children:'regularization = "auto"'}),", the optimizer automatically finds a suitable regularization strength that selects approximately ",(0,t.jsx)(n.code,{children:"regularization_ratio * num_learners"})," weak learners for the final ensemble, rather than fixing the penalty manually. This provides a more convenient interface for managing the balance between sparsity and ensemble size."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'regularization_type = "alpha"'})," defines how the penalty is applied. Unlike ",(0,t.jsx)(n.code,{children:"onsite"}),", which is unbounded ",(0,t.jsx)(n.code,{children:"[0, \u221e]"}),", ",(0,t.jsx)(n.code,{children:"alpha"})," is bounded between ",(0,t.jsx)(n.code,{children:"[0, 1]"}),", making it easier to tune and interpret. The parameter controls the trade-off between individual and pairwise penalties, offering a smoother configuration range."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"regularization_desired_ratio \u2248 0.82"})," specifies the target proportion of learners to keep active after regularization \u2014 here, around 82% of learners are retained, trimming the weakest 18% automatically."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"While adaptive regularization simplifies configuration and helps maintain a balanced ensemble, it does not necessarily guarantee better or more stable performance. The actual quality depends on selecting an appropriate regularization parameter, and fine-tuning it through cross-validation can be computationally expensive. The main advantage lies in improved usability and interpretability rather than direct accuracy gains."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Problem scale and regularization\nNUM_LEARNERS = 60\nREGULARIZATION = "auto"\nREGULARIZATION_TYPE = "alpha"\nREGULARIZATION_RATIO = 0.82\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ----- Quantum-enhanced ensemble on IBM hardware -----\nprint("\\n-- Submitting quantum-enhanced ensemble job --")\njob_3 = singularity.run(\n    action="create_fit_predict",\n    name="grid_stability_qeec",\n    quantum_classifier="QuantumEnhancedEnsembleClassifier",\n    num_learners=NUM_LEARNERS,\n    regularization=REGULARIZATION,\n    regularization_type=REGULARIZATION_TYPE,\n    regularization_desired_ratio=REGULARIZATION_RATIO,\n    optimizer_options=optimizer_options,  # from Step 2\n    backend_name=backend,  # least-busy compatible backend\n    instance=IBM_INSTANCE_QUANTUM,\n    random_state=RANDOM_STATE,\n    X_train=X_train_bal,\n    y_train=y_train_bal,\n    X_test=X_test,\n    fit_params={"validation_data": (X_val, y_val)},\n    options={"save": False},\n)\nresult_3 = job_3.result()\nprint("Action status:", result_3.get("status"))\nprint("Action message:", result_3.get("message"))\nprint("Metadata:", result_3.get("metadata"))\nqeec_pred_job_3 = np.array(result_3["data"]["predictions"])\n_ = evaluate_predictions(qeec_pred_job_3, y_test)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"-- Submitting quantum-enhanced ensemble job --\nAction status: ok\nAction message: Classifier created, fitted, and predicted.\nMetadata: {'resource_usage': {'RUNNING: MAPPING': {'CPU_TIME': 1387.7451872825623}, 'RUNNING: WAITING_QPU': {'CPU_TIME': 95.41597843170166}, 'RUNNING: POST_PROCESSING': {'CPU_TIME': 171.78878355026245}, 'RUNNING: EXECUTING_QPU': {'QPU_TIME': 1146.5584812164307}}}\nAccuracy: 0.908\nPrecision: 1.0\nRecall: 0.908\nF1: 0.9517819706498952\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'status_3 = job_3.status()\nprint("\\nQuantum job status:", status_3)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Quantum job status: DONE\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-4-post-process-and-return-result-in-desired-classical-format",children:"Step 4: Post-process and return result in desired classical format"}),"\n",(0,t.jsxs)(n.p,{children:["We now post-process outputs from both the classical and quantum runs, converting them into a consistent format for downstream evaluation. This step compares predictive quality using standard metrics - accuracy, precision, recall, and F1 - and analyzes how ensemble width (",(0,t.jsx)(n.code,{children:"num_learners"}),") and sparsity control (",(0,t.jsx)(n.code,{children:"regularization"}),") influence both performance and computational behavior."]}),"\n",(0,t.jsxs)(n.p,{children:["The classical AdaBoost baseline provides a compact and stable reference for small-scale learning. It performs well with limited ensembles and negligible compute overhead, reflecting the strength of traditional boosting when the hypothesis space is still tractable. The quantum configurations (",(0,t.jsx)(n.code,{children:"qeec_pred_job_1"}),", ",(0,t.jsx)(n.code,{children:"qeec_pred_job_2"}),", and ",(0,t.jsx)(n.code,{children:"qeec_pred_job_3"}),") extend this baseline by embedding the ensemble-selection process within a variational quantum optimization loop. This allows the system to explore exponentially large subsets of learners simultaneously in superposition, addressing the combinatorial nature of ensemble selection more efficiently as scale increases."]}),"\n",(0,t.jsxs)(n.p,{children:["Results show that increasing ",(0,t.jsx)(n.code,{children:"num_learners"})," from 10 to 30 improves recall and F1, confirming that a wider ensemble captures richer interactions among weak learners. The gain is sublinear on current hardware - each additional learner yields smaller accuracy increments - but the underlying scaling behavior remains favorable because the quantum optimizer can search broader configuration spaces without the exponential blow-up typical of classical subset selection. Regularization introduces additional nuance: a fixed \u03bb=7 enforces consistent sparsity and stabilizes convergence, whereas adaptive \u03b1-regularization automatically tunes sparsity based on correlations between learners. This dynamic pruning often achieves slightly higher F1 for the same qubit width, balancing model complexity and generalization."]}),"\n",(0,t.jsxs)(n.p,{children:['When compared directly with the AdaBoost baseline, the smallest quantum configuration (L=10) reproduces similar accuracy, validating the hybrid pipeline\u2019s correctness. At larger widths, quantum variants - especially with auto-regularization - begin to surpass the classical baseline modestly, showing improved recall and F1 without linear growth in computational cost. These improvements do not indicate immediate "quantum advantage" but rather ',(0,t.jsx)(n.strong,{children:"scaling efficiency"}),": the quantum optimizer maintains tractable performance as the ensemble expands, where a classical approach would face exponential growth in subset-selection complexity."]}),"\n",(0,t.jsx)(n.p,{children:"In practice:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Use the ",(0,t.jsx)(n.strong,{children:"classical baseline"})," for quick validation and benchmarking on small datasets."]}),"\n",(0,t.jsxs)(n.li,{children:["Apply ",(0,t.jsx)(n.strong,{children:"quantum ensembles"})," when model width or feature complexity grows\u2014QAOA-based search scales more gracefully in those regimes."]}),"\n",(0,t.jsxs)(n.li,{children:["Employ ",(0,t.jsx)(n.strong,{children:"adaptive \u03b1-regularization"})," to maintain sparsity and generalization without increasing circuit width."]}),"\n",(0,t.jsx)(n.li,{children:"Monitor QPU time and depth to balance quality gains against near-term hardware constraints."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Together, these experiments show that quantum-optimized ensembles complement classical methods: they reproduce baseline accuracy at small scales while offering a path to efficient scaling on larger, combinatorial learning problems. As hardware improves, these scaling advantages are expected to compound, extending the feasible size and depth of ensemble-based models beyond what is classically practical."}),"\n",(0,t.jsx)(n.h3,{id:"evaluate-metrics-for-each-configuration",children:"Evaluate metrics for each configuration"}),"\n",(0,t.jsxs)(n.p,{children:["We now evaluate all configurations - the classical AdaBoost baseline and the three quantum ensembles - using the ",(0,t.jsx)(n.code,{children:"evaluate_predictions"})," helper to compute accuracy, precision, recall, and F1 on the same test set. This comparison clarifies how quantum optimization scales relative to the classical approach: at small widths, both perform similarly; as ensembles grow, the quantum method can explore larger hypothesis spaces more efficiently. The resulting table captures these trends in a consistent, quantitative form."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'results = []\n\n# Classical baseline\nacc_b, prec_b, rec_b, f1_b = evaluate_predictions(baseline_pred, y_test)\nresults.append(\n    {\n        "Config": "AdaBoost (Classical)",\n        "Accuracy": acc_b,\n        "Precision": prec_b,\n        "Recall": rec_b,\n        "F1": f1_b,\n    }\n)\n\n# Quantum runs\nfor label, preds in [\n    ("QEEC L=10, reg=7", qeec_pred_job_1),\n    ("QEEC L=30, reg=7", qeec_pred_job_2),\n    (f"QEEC L=60, reg=auto (\u03b1={REGULARIZATION_RATIO})", qeec_pred_job_3),\n]:\n    acc, prec, rec, f1 = evaluate_predictions(preds, y_test)\n    results.append(\n        {\n            "Config": label,\n            "Accuracy": acc,\n            "Precision": prec,\n            "Recall": rec,\n            "F1": f1,\n        }\n    )\n\ndf_results = pd.DataFrame(results)\ndf_results\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Accuracy: 0.7893333333333333\nPrecision: 1.0\nRecall: 0.7893333333333333\nF1: 0.8822652757078987\nAccuracy: 0.868\nPrecision: 1.0\nRecall: 0.868\nF1: 0.9293361884368309\nAccuracy: 0.8946666666666667\nPrecision: 1.0\nRecall: 0.8946666666666667\nF1: 0.944405348346235\nAccuracy: 0.908\nPrecision: 1.0\nRecall: 0.908\nF1: 0.9517819706498952\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Config  Accuracy  Precision    Recall        F1\n0          AdaBoost (Classical)  0.789333        1.0  0.789333  0.882265\n1              QEEC L=10, reg=7  0.868000        1.0  0.868000  0.929336\n2              QEEC L=30, reg=7  0.894667        1.0  0.894667  0.944405\n3  QEEC L=60, reg=auto (\u03b1=0.82)  0.908000        1.0  0.908000  0.951782\n"})}),"\n",(0,t.jsx)(n.h3,{id:"visualize-quality-trends-across-configurations",children:"Visualize quality trends across configurations"}),"\n",(0,t.jsxs)(n.p,{children:["The grouped bar chart below compares ",(0,t.jsx)(n.strong,{children:"Accuracy"})," and ",(0,t.jsx)(n.strong,{children:"F1"})," across the classical baseline and the quantum ensembles (",(0,t.jsx)(n.code,{children:"L=10"}),", ",(0,t.jsx)(n.code,{children:"L=30"}),", and ",(0,t.jsx)(n.code,{children:"L=60 auto-\u03b1"}),"). It illustrates how accuracy stabilizes while F1 gradually improves as quantum ensemble width increases, demonstrating that the hybrid method sustains performance scaling without the exponential cost growth typical of classical subset selection."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'x = np.arange(len(df_results))\nwidth = 0.35\nplt.figure(figsize=(7.6, 4.6))\nplt.bar(x - width / 2, df_results["Accuracy"], width=width, label="Accuracy")\nplt.bar(x + width / 2, df_results["F1"], width=width, label="F1")\nplt.xticks(x, df_results["Config"], rotation=10)\nplt.ylabel("Score")\nplt.title("Classical vs Quantum ensemble performance")\nplt.legend()\nplt.ylim(0, 1.0)\nplt.tight_layout()\nplt.show()\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"Output of the previous code cell",src:a(17991).A+"",width:"742",height:"449"})}),"\n",(0,t.jsx)(n.h3,{id:"interpretation",children:"Interpretation"}),"\n",(0,t.jsx)(n.p,{children:"The plot confirms the expected scaling pattern. The classical AdaBoost performs strongly for smaller ensembles but becomes increasingly costly to scale as the number of weak learners grows, because its subset-selection problem expands combinatorially. The quantum-enhanced models replicate classical accuracy at low widths and begin to surpass it as ensemble size increases, especially under adaptive \u03b1-regularization. This reflects the quantum optimizer\u2019s ability to sample and evaluate many candidate subsets in parallel through superposition, maintaining tractable search even at higher widths. While current hardware overhead offsets some of the theoretical gains, the trend illustrates the scaling efficiency advantage of the quantum formulation. In practical terms, the classical method remains preferable for lightweight benchmarks, while quantum-enhanced ensembles become advantageous as model dimensionality and ensemble size expand, offering better trade-offs between accuracy, generalization, and computational growth."}),"\n",(0,t.jsx)(n.h2,{id:"appendix-scaling-benefits-and-enhancements",children:"Appendix: Scaling benefits and enhancements"}),"\n",(0,t.jsxs)(n.p,{children:["The scalability advantage of the ",(0,t.jsx)(n.code,{children:"QuantumEnhancedEnsembleClassifier"})," arises from how the ensemble-selection process maps to quantum optimization.\nClassical ensemble learning methods, such as AdaBoost or random forests, become computationally expensive as the number of weak learners increases because selecting the optimal subset is a combinatorial problem that scales exponentially."]}),"\n",(0,t.jsx)(n.p,{children:"In contrast, the quantum formulation \u2014 implemented here via the Quantum Approximate Optimization Algorithm (QAOA) \u2014 can explore these exponentially large search spaces more efficiently by evaluating multiple configurations in superposition.\nAs a result, the training time does not grow significantly with the number of learners, allowing the model to remain efficient even as ensemble width increases."}),"\n",(0,t.jsx)(n.p,{children:"While current hardware introduces some noise and depth limitations, this workflow demonstrates a near-term hybrid approach where classical and quantum components cooperate: the quantum optimizer provides a better initialization landscape for the classical loop, improving convergence and final model quality.\nAs quantum processors evolve, these scalability benefits are expected to extend to larger datasets, broader ensembles, and deeper circuit depths."}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/guides/functions",children:"Introduction to Qiskit Functions"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/guides/multiverse-computing-singularity",children:"Multiverse Computing Singularity Machine Learning"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"tutorial-survey",children:"Tutorial survey"}),"\n",(0,t.jsx)(n.p,{children:"Please take a minute to provide feedback on this tutorial. Your insights will help us improve our content offerings and user experience."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://your.feedback.ibm.com/jfe/form/SV_3BLFkNVEuh0QBWm",children:"Link to survey"})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},17991(e,n,a){a.d(n,{A:()=>i});const i="data:image/avif;base64,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"},28453(e,n,a){a.d(n,{R:()=>r,x:()=>l});var i=a(96540);const t={},s=i.createContext(t);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);