"use strict";(globalThis.webpackChunkdoqumentation=globalThis.webpackChunkdoqumentation||[]).push([[2421],{27058(e,i,n){n.r(i),n.d(i,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>d,metadata:()=>s,toc:()=>r});const s=JSON.parse('{"id":"guides/qiskit-code-assistant-local","title":"Use Qiskit Code Assistant in local mode","description":"Learn how to deploy and use the Qiskit Code Assistant model locally.","source":"@site/i18n/tl/docusaurus-plugin-content-docs/current/guides/qiskit-code-assistant-local.mdx","sourceDirName":"guides","slug":"/guides/qiskit-code-assistant-local","permalink":"/guides/qiskit-code-assistant-local","draft":false,"unlisted":false,"editUrl":"https://github.com/JanLahmann/doQumentation/tree/main/docs/guides/qiskit-code-assistant-local.mdx","tags":[],"version":"current","frontMatter":{"title":"Use Qiskit Code Assistant in local mode","description":"Learn how to deploy and use the Qiskit Code Assistant model locally."},"sidebar":"tutorialsSidebar","previous":{"title":"Use Qiskit Code Assistant in VS Code","permalink":"/guides/qiskit-code-assistant-vscode"},"next":{"title":"Qiskit Code Assistant - OpenAI API compatibility","permalink":"/guides/qiskit-code-assistant-openai-api"}}');var t=n(74848),l=n(28453);const d={title:"Use Qiskit Code Assistant in local mode",description:"Learn how to deploy and use the Qiskit Code Assistant model locally."},o="Use Qiskit Code Assistant in local mode",a={},r=[{value:"Quick start (recommended)",id:"quick-start-recommended",level:2},{value:"VS Code extension setup",id:"vs-code-extension-setup",level:3},{value:"JupyterLab extension setup",id:"jupyterlab-extension-setup",level:3},{value:"Available models",id:"available-models",level:2},{value:"Current models",id:"current-models",level:3},{value:"GGUF models (recommended for personal environments/laptops)",id:"gguf-models-recommended-for-personal-environmentslaptops",level:3},{value:"Qiskit versions used for training",id:"qiskit-versions-used-for-training",level:3},{value:"Deprecated models",id:"deprecated-models",level:3},{value:"Advanced setup",id:"advanced-setup",level:2},{value:"Using the Ollama application",id:"using-the-ollama-application",level:3},{value:"Install Ollama",id:"install-ollama",level:4},{value:"Set up Ollama using the Hugging Face Hub integration",id:"set-up-ollama-using-the-hugging-face-hub-integration",level:4},{value:"Set up Ollama with a manually downloaded Qiskit Code Assistant GGUF model",id:"set-up-ollama-with-a-manually-downloaded-qiskit-code-assistant-gguf-model",level:4},{value:"Run the Qiskit Code Assistant model manually downloaded in Ollama",id:"run-the-qiskit-code-assistant-model-manually-downloaded-in-ollama",level:4},{value:"Use the <code>llama.cpp</code> library",id:"use-the-llamacpp-library",level:3},{value:"Advanced parameters",id:"advanced-parameters",level:4},{value:"Connect with the Qiskit Code Assistant VS Code extension",id:"connect-with-the-qiskit-code-assistant-vs-code-extension",level:3},{value:"Connect with the Qiskit Code Assistant JupyterLab extension",id:"connect-with-the-qiskit-code-assistant-jupyterlab-extension",level:3}];function c(e){const i={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components},{Admonition:n,DefinitionTooltip:s,Details:d}=i;return n||p("Admonition",!0),s||p("DefinitionTooltip",!0),d||p("Details",!0),(0,t.jsxs)(t.Fragment,{children:["\n",(0,t.jsx)(i.admonition,{title:"Hindi pa naisalin",type:"note",children:(0,t.jsx)(i.p,{children:"Ang pahinang ito ay hindi pa naisalin. Nakikita mo ang orihinal na bersyon sa Ingles."})}),"\n","\n",(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"use-qiskit-code-assistant-in-local-mode",children:"Use Qiskit Code Assistant in local mode"})}),"\n",(0,t.jsx)(i.p,{children:"Learn how to install, configure, and use any of Qiskit Code Assistant models on your local machine."}),"\n",(0,t.jsx)(n,{type:"note",title:"Notes",children:(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Qiskit Code Assistant is in preview release status and is subject to change."}),"\n",(0,t.jsxs)(i.li,{children:["If you have feedback or want to contact the developer team, use the ",(0,t.jsx)(i.a,{href:"https://qiskit.enterprise.slack.com/archives/C07LYA6PL83",children:"Qiskit Slack Workspace channel"})," or the related public GitHub repositories."]}),"\n"]})}),"\n",(0,t.jsx)(i.h2,{id:"quick-start-recommended",children:"Quick start (recommended)"}),"\n",(0,t.jsxs)(i.p,{children:["The easiest way to get started with Qiskit Code Assistant in local mode is to use the automated setup scripts for either the VS Code or JupyterLab extension. These scripts will automatically install ",(0,t.jsx)(i.a,{href:"https://ollama.com/",children:"Ollama"})," to run the LLMs, download the recommended model, and configure the extension for you."]}),"\n",(0,t.jsx)(i.h3,{id:"vs-code-extension-setup",children:"VS Code extension setup"}),"\n",(0,t.jsx)(i.p,{children:"Run the following command in your terminal:"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-bash",children:"bash <(curl -fsSL https://raw.githubusercontent.com/Qiskit/qiskit-code-assistant-vscode/main/setup_local.sh)\n"})}),"\n",(0,t.jsx)(i.p,{children:"This script performs the following steps:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Install Ollama (if not already installed)"}),"\n",(0,t.jsx)(i.li,{children:"Download and configure the recommended Qiskit Code Assistant model"}),"\n",(0,t.jsx)(i.li,{children:"Set up the VS Code extension to work with your local deployment"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"jupyterlab-extension-setup",children:"JupyterLab extension setup"}),"\n",(0,t.jsx)(i.p,{children:"Run the following command in your terminal:"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-bash",children:"bash <(curl -fsSL https://raw.githubusercontent.com/Qiskit/qiskit-code-assistant-jupyterlab/main/setup_local.sh)\n"})}),"\n",(0,t.jsx)(i.p,{children:"This script will:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Install Ollama (if not already installed)"}),"\n",(0,t.jsx)(i.li,{children:"Download and configure the recommended Qiskit Code Assistant model"}),"\n",(0,t.jsx)(i.li,{children:"Set up the JupyterLab extension to work with your local deployment"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"available-models",children:"Available models"}),"\n",(0,t.jsx)(i.h3,{id:"current-models",children:"Current models"}),"\n",(0,t.jsx)(i.p,{children:"These are the latest recommended models for use with Qiskit Code Assistant:"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/mistral-small-3.2-24b-qiskit",children:"Qiskit/mistral-small-3.2-24b-qiskit"})," - Released October 2025"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/qwen2.5-coder-14b-qiskit",children:"qiskit/qwen2.5-coder-14b-qiskit"})," - Released June 2025"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/granite-3.3-8b-qiskit",children:"qiskit/granite-3.3-8b-qiskit"})," - Released June 2025"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/granite-3.2-8b-qiskit",children:"qiskit/granite-3.2-8b-qiskit"})," - Released June 2025"]}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"gguf-models-recommended-for-personal-environmentslaptops",children:"GGUF models (recommended for personal environments/laptops)"}),"\n",(0,t.jsx)(i.p,{children:"GGUF format models are optimized for local use and require fewer computational resources:"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/mistral-small-3.2-24b-qiskit-GGUF",children:"mistral-small-3.2-24b-qiskit-GGUF"})," \u2013 Released ",(0,t.jsx)(i.strong,{children:"October 2025"}),(0,t.jsx)(i.br,{}),"\n",(0,t.jsxs)(i.em,{children:["Trained with Qiskit data up to version ",(0,t.jsx)(i.strong,{children:"2.1"})]})]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/qwen2.5-coder-14b-qiskit-GGUF",children:"qiskit/qwen2.5-coder-14b-qiskit-GGUF"})," \u2013 Released ",(0,t.jsx)(i.strong,{children:"June 2025"}),(0,t.jsx)(i.br,{}),"\n",(0,t.jsxs)(i.em,{children:["Trained with Qiskit data up to version ",(0,t.jsx)(i.strong,{children:"2.0"})]})]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/granite-3.3-8b-qiskit-GGUF",children:"qiskit/granite-3.3-8b-qiskit-GGUF"})," \u2013 Released ",(0,t.jsx)(i.strong,{children:"June 2025"}),(0,t.jsx)(i.br,{}),"\n",(0,t.jsxs)(i.em,{children:["Trained with Qiskit data up to version ",(0,t.jsx)(i.strong,{children:"2.0"})]})]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/granite-3.2-8b-qiskit-GGUF",children:"qiskit/granite-3.2-8b-qiskit-GGUF"})," \u2013 Released ",(0,t.jsx)(i.strong,{children:"June 2025"}),(0,t.jsx)(i.br,{}),"\n",(0,t.jsxs)(i.em,{children:["Trained with Qiskit data up to version ",(0,t.jsx)(i.strong,{children:"2.0"})]})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(i.p,{children:["The Open Source Qiskit Code Assistant models are available in ",(0,t.jsx)(s,{definition:"Safetensors is a file format designed specifically for storing machine learning model weights and tensors in a secure and efficient manner.",children:"safetensors"})," or ",(0,t.jsx)(s,{definition:"GGUF is a binary format that is designed for quickly loading and saving models, and for readability.",children:"GGUF file format"})," and can be downloaded from the Hugging Face as explained below."]}),"\n",(0,t.jsx)(i.h3,{id:"qiskit-versions-used-for-training",children:"Qiskit versions used for training"}),"\n",(0,t.jsxs)(i.table,{children:[(0,t.jsx)(i.thead,{children:(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.th,{children:(0,t.jsx)(i.strong,{children:"Model"})}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:(0,t.jsx)(i.strong,{children:"Benchmark Metrics"})}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:"\xa0"}),(0,t.jsx)(i.th,{children:(0,t.jsx)(i.strong,{children:"Release date"})}),(0,t.jsx)(i.th,{children:(0,t.jsx)(i.strong,{children:"Trained on Qiskit version"})})]})}),(0,t.jsxs)(i.tbody,{children:[(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{children:"\xa0"}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"QiskitHumanEval-Hard"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"QiskitHumanEval"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"HumanEval"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"ASDiv"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"MathQA"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"SciQ"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"MBPP"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"IFEval"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"CrowsPairs (English)"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"TruthfulQA (MC1 acc)"})}),(0,t.jsx)(i.td,{children:"\xa0"}),(0,t.jsx)(i.td,{children:"\xa0"})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"mistral-small-3.2-24b-qiskit"})}),(0,t.jsx)(i.td,{children:"32.45"}),(0,t.jsx)(i.td,{children:"47.02"}),(0,t.jsx)(i.td,{children:"77.49"}),(0,t.jsx)(i.td,{children:"3.77"}),(0,t.jsx)(i.td,{children:"49.68"}),(0,t.jsx)(i.td,{children:"97.50"}),(0,t.jsx)(i.td,{children:"64.00"}),(0,t.jsx)(i.td,{children:"48.44"}),(0,t.jsx)(i.td,{children:"67.08"}),(0,t.jsx)(i.td,{children:"39.41"}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"January 2026"})}),(0,t.jsx)(i.td,{children:(0,t.jsx)(i.strong,{children:"2.2"})})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{children:"qwen2.5-coder-14b-qiskit"}),(0,t.jsx)(i.td,{children:"25.17"}),(0,t.jsx)(i.td,{children:"49.01"}),(0,t.jsx)(i.td,{children:"91.46"}),(0,t.jsx)(i.td,{children:"4.21"}),(0,t.jsx)(i.td,{children:"53.90"}),(0,t.jsx)(i.td,{children:"97.00"}),(0,t.jsx)(i.td,{children:"77.60"}),(0,t.jsx)(i.td,{children:"49.64"}),(0,t.jsx)(i.td,{children:"65.18"}),(0,t.jsx)(i.td,{children:"37.82"}),(0,t.jsx)(i.td,{children:"June 2025"}),(0,t.jsx)(i.td,{children:"2.0"})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{children:"granite-3.3-8b-qiskit"}),(0,t.jsx)(i.td,{children:"14.57"}),(0,t.jsx)(i.td,{children:"27.15"}),(0,t.jsx)(i.td,{children:"62.80"}),(0,t.jsx)(i.td,{children:"0.48"}),(0,t.jsx)(i.td,{children:"38.66"}),(0,t.jsx)(i.td,{children:"93.30"}),(0,t.jsx)(i.td,{children:"52.40"}),(0,t.jsx)(i.td,{children:"59.71"}),(0,t.jsx)(i.td,{children:"59.75"}),(0,t.jsx)(i.td,{children:"39.05"}),(0,t.jsx)(i.td,{children:"June 2025"}),(0,t.jsx)(i.td,{children:"2.0"})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{children:"granite-3.2-8b-qiskit"}),(0,t.jsx)(i.td,{children:"9.93"}),(0,t.jsx)(i.td,{children:"24.50"}),(0,t.jsx)(i.td,{children:"57.32"}),(0,t.jsx)(i.td,{children:"0.09"}),(0,t.jsx)(i.td,{children:"41.41"}),(0,t.jsx)(i.td,{children:"96.30"}),(0,t.jsx)(i.td,{children:"51.80"}),(0,t.jsx)(i.td,{children:"60.79"}),(0,t.jsx)(i.td,{children:"66.79"}),(0,t.jsx)(i.td,{children:"40.51"}),(0,t.jsx)(i.td,{children:"June 2025"}),(0,t.jsx)(i.td,{children:"2.0"})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{children:"granite-8b-qiskit-rc-0.10"}),(0,t.jsx)(i.td,{children:"15.89"}),(0,t.jsx)(i.td,{children:"38.41"}),(0,t.jsx)(i.td,{children:"59.76"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"February 2025"}),(0,t.jsx)(i.td,{children:"1.3"})]}),(0,t.jsxs)(i.tr,{children:[(0,t.jsx)(i.td,{children:"granite-8b-qiskit"}),(0,t.jsx)(i.td,{children:"17.88"}),(0,t.jsx)(i.td,{children:"44.37"}),(0,t.jsx)(i.td,{children:"53.66"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"\u2014"}),(0,t.jsx)(i.td,{children:"November 2024"}),(0,t.jsx)(i.td,{children:"1.2"})]})]})]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.em,{children:"Note: All models listed in the benchmark table were evaluated using their respective system prompt, defined in their Hugging Face model."})}),"\n",(0,t.jsx)(i.h3,{id:"deprecated-models",children:"Deprecated models"}),"\n",(0,t.jsx)(i.p,{children:"These models are no longer actively maintained but remain available:"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/granite-8b-qiskit-rc-0.10",children:"qiskit/granite-8b-qiskit-rc-0.10"})," - Released February 2025 (deprecated)"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/granite-8b-qiskit",children:"qiskit/granite-8b-qiskit"})," - Released November 2024 (deprecated)"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"advanced-setup",children:"Advanced setup"}),"\n",(0,t.jsx)(i.p,{children:"If you prefer to manually configure your local setup or need more control over the installation process, expand the sections below."}),"\n",(0,t.jsxs)(d,{children:[(0,t.jsx)("summary",{children:"Download from the Hugging Face website"}),(0,t.jsx)(i.p,{children:"Follow these steps to download any Qiskit Code Assistant-related model from the Hugging Face website:"}),(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsx)(i.li,{children:"Navigate to the desired Qiskit model page on Hugging Face."}),"\n",(0,t.jsxs)(i.li,{children:["Go to the ",(0,t.jsx)(i.strong,{children:"Files and Versions"})," tab and download the safetensors or GGUF model files."]}),"\n"]})]}),"\n",(0,t.jsxs)(d,{children:[(0,t.jsx)("summary",{children:"Download using the Hugging Face CLI"}),(0,t.jsx)(i.p,{children:"To download any of the available Qiskit Code Assistant models using the Hugging Face CLI, follow these steps:"}),(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["Install the ",(0,t.jsx)(i.a,{href:"https://huggingface.co/docs/huggingface_hub/main/en/guides/cli",children:"Hugging Face CLI"})]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Log in to your Hugging Face account"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{children:"huggingface-cli login\n"})}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Download the model you prefer from the previous list"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{children:"huggingface-cli download <HF REPO NAME> <MODEL PATH> --local-dir <LOCAL PATH>\n"})}),"\n"]}),"\n"]})]}),"\n",(0,t.jsxs)(d,{children:[(0,t.jsx)("summary",{children:"Manually deploy the Qiskit Code Assistant models in local through Ollama"}),(0,t.jsxs)(i.p,{children:["There are multiple ways to deploy and interact with the downloaded Qiskit Code Assistant model. This guide demonstrates using ",(0,t.jsx)(i.a,{href:"https://ollama.com",children:"Ollama"})," as follows: either with the ",(0,t.jsx)(i.a,{href:"#using-the-ollama-application",children:"Ollama application"})," by using the Hugging Face Hub integration or local model, or with the ",(0,t.jsxs)(i.a,{href:"#use-llama-cpp-python",children:[(0,t.jsx)(i.code,{children:"llama-cpp-python"})," package"]}),"."]}),(0,t.jsx)(i.h3,{id:"using-the-ollama-application",children:"Using the Ollama application"}),(0,t.jsx)(i.p,{children:"The Ollama application provides a simple solution to run the LLMs locally. It is easy to use, with a CLI that makes the whole setup process, model management, and interaction fairly straightforward. It\u2019s ideal for quick experimentation and for users that want fewer technical details to handle."}),(0,t.jsx)(i.h4,{id:"install-ollama",children:"Install Ollama"}),(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["Download the ",(0,t.jsx)(i.a,{href:"https://ollama.com/download",children:"Ollama application"})]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Install the downloaded file"}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Launch the installed Ollama application"}),"\n",(0,t.jsxs)(n,{type:"info",children:["The application is running successfully when the Ollama icon appears in the desktop menu bar. You can also verify the service is running by going to ",(0,t.jsx)(i.code,{children:"http://localhost:11434/"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Try Ollama in your terminal and start running models. For example:"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{children:"ollama run hf.co/Qiskit/Qwen2.5-Coder-14B-Qiskit\n"})}),"\n"]}),"\n"]}),(0,t.jsx)(i.h4,{id:"set-up-ollama-using-the-hugging-face-hub-integration",children:"Set up Ollama using the Hugging Face Hub integration"}),(0,t.jsxs)(i.p,{children:["The ",(0,t.jsx)(i.a,{href:"https://huggingface.co/docs/hub/ollama",children:"Ollama/Hugging Face Hub integration"})," provides a way to interact with models hosted on the Hugging Face Hub without needing to create a new modelfile nor manually downloading the GGUF or safetensors files. The default ",(0,t.jsx)(i.code,{children:"template"})," and ",(0,t.jsx)(i.code,{children:"params"})," files are already included for the model on the Hugging Face Hub."]}),(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Make sure the Ollama application is running."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["Go the desired model page, and copy the URL. For example, ",(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/Qwen2.5-Coder-14B-Qiskit-GGUF",children:"https://huggingface.co/Qiskit/Qwen2.5-Coder-14B-Qiskit-GGUF"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"From your terminal, run the command:"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{children:"ollama run hf.co/Qiskit/Qwen2.5-Coder-14B-Qiskit\n"})}),"\n"]}),"\n"]}),(0,t.jsxs)(i.p,{children:["You can use the ",(0,t.jsx)(i.code,{children:"hf.co/Qiskit/Qwen2.5-Coder-14B-Qiskit"})," model or any of the other currently recommended GGUF official models ",(0,t.jsx)(i.code,{children:"hf.co/Qiskit/mistral-small-3.2-24b-qiskit-GGUF"})," or ",(0,t.jsx)(i.code,{children:"hf.co/Qiskit/granite-3.3-8b-qiskit-GGUF"}),"."]}),(0,t.jsx)(i.h4,{id:"set-up-ollama-with-a-manually-downloaded-qiskit-code-assistant-gguf-model",children:"Set up Ollama with a manually downloaded Qiskit Code Assistant GGUF model"}),(0,t.jsxs)(i.p,{children:["If you have manually downloaded a GGUF model such as ",(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/Qwen2.5-Coder-14B-Qiskit-GGUF",children:"https://huggingface.co/Qiskit/Qwen2.5-Coder-14B-Qiskit-GGUF"})," and you want to experiment with different templates and parameters, you can follow these steps to load it into your local Ollama application."]}),(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["Create a ",(0,t.jsx)(i.code,{children:"Modelfile"})," entering the following content and be sure to update ",(0,t.jsx)(i.code,{children:"<PATH-TO-GGUF-FILE>"})," to the actual path of your downloaded model."]}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{children:'FROM <PATH-TO-GGUF-FILE>\nTEMPLATE """{{ if .System }}\nSystem:\n{{ .System }}\n\n{{ end }}{{ if .Prompt }}Question:\n{{ .Prompt }}\n\n{{ end }}Answer:\n```python{{ .Response }}\n"""\n\nPARAMETER stop "Question:"\nPARAMETER stop "Answer:"\nPARAMETER stop "System:"\nPARAMETER stop "```"\n\nPARAMETER temperature 0\nPARAMETER top_k 1\n'})}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["Run the following command to create a custom model instance based on the ",(0,t.jsx)(i.code,{children:"Modelfile"}),"."]}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{children:"ollama create Qwen2.5-Coder-14B-Qiskit -f ./path-to-model-file\n"})}),"\n",(0,t.jsx)(n,{type:"note",children:"This process may take some time for Ollama to read the model file, initialize the model instance, and configure it according to the specifications provided."}),"\n"]}),"\n"]}),(0,t.jsx)(i.h4,{id:"run-the-qiskit-code-assistant-model-manually-downloaded-in-ollama",children:"Run the Qiskit Code Assistant model manually downloaded in Ollama"}),(0,t.jsxs)(i.p,{children:["After the ",(0,t.jsx)(i.code,{children:"Qwen2.5-Coder-14B-Qiskit"})," model has been set up in Ollama, run the following command to launch the model and interact with it in the terminal (in chat mode)."]}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{children:"ollama run Qwen2.5-Coder-14B-Qiskit\n"})}),(0,t.jsx)(i.p,{children:"Some useful commands:"}),(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"ollama list"})," - List models on your computer"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"ollama rm Qwen2.5-Coder-14B-Qiskit"})," - Delete the model"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"ollama show Qwen2.5-Coder-14B-Qiskit"})," - Show model information"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"ollama stop Qwen2.5-Coder-14B-Qiskit"})," - Stop a model that is currently running"]}),"\n",(0,t.jsxs)(i.li,{children:[(0,t.jsx)(i.code,{children:"ollama ps"})," - List which models are currently loaded"]}),"\n"]})]}),"\n",(0,t.jsxs)(d,{children:[(0,t.jsxs)("summary",{children:["Manually deploy the Qiskit Code Assistant models in local through the ",(0,t.jsx)(i.code,{children:"llama-cpp-python"})," package"]}),(0,t.jsx)("span",{id:"use-llama-cpp-python"}),(0,t.jsxs)(i.p,{children:["An alternative to the Ollama application is the ",(0,t.jsx)(i.code,{children:"llama-cpp-python"})," package, which is a Python binding for ",(0,t.jsx)(i.code,{children:"llama.cpp"}),". It gives you more control and flexibility to run the GGUF model locally, and is ideal for users who wish to integrate the local model in their workflows and Python applications."]}),(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["Install ",(0,t.jsx)(i.a,{href:"https://pypi.org/project/llama-cpp-python/",children:(0,t.jsx)(i.code,{children:"llama-cpp-python"})})]}),"\n",(0,t.jsxs)(i.li,{children:["Interact with the model from within your application using ",(0,t.jsx)(i.code,{children:"llama_cpp"}),". For example:"]}),"\n"]}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-python",children:'from llama_cpp import Llama\n\nmodel_path = <PATH-TO-GGUF-FILE>\n\nmodel = Llama(\n        model_path,\n        seed=17,\n        n_ctx=10000,\n        n_gpu_layers=37, # to offload in gpu, but put 0 if all in cpu\n    )\n\ninput = \'Generate a quantum circuit with 2 qubits\'\nraw_pred = model(input)["choices"][0]["text"]\n'})}),(0,t.jsx)(i.p,{children:"You can also add text generation parameters to the model to customize the inference:"}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-python",children:'generation_kwargs = {\n        "max_tokens": 512,\n        "echo": False, # Echo the prompt in the output\n        "top_k": 1\n    }\n\nraw_pred = model(input, **generation_kwargs)["choices"][0]["text"]\n'})})]}),"\n",(0,t.jsxs)(d,{children:[(0,t.jsx)("summary",{children:"Manually deploy the Qiskit Code Assistant models in local through llama.cpp"}),(0,t.jsxs)(i.h3,{id:"use-the-llamacpp-library",children:["Use the ",(0,t.jsx)(i.code,{children:"llama.cpp"})," library"]}),(0,t.jsxs)(i.p,{children:["Another alternative is to use ",(0,t.jsx)(i.code,{children:"llama.cpp"}),", an open-source library for performing LLM inference on a CPU with minimal setup.\nIt provides low-level control over the model execution and is typically run from the command line, pointing to a local GGUF model file."]}),(0,t.jsxs)(i.p,{children:["There are several ways to install ",(0,t.jsx)(i.code,{children:"llama.cpp"})," on your machine:"]}),(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["Install llama.cpp using ",(0,t.jsx)(i.a,{href:"https://github.com/ggml-org/llama.cpp/blob/master/docs/install.md",children:"brew, nix, or winget"})]}),"\n",(0,t.jsxs)(i.li,{children:["Run with Docker: See out the ",(0,t.jsxs)(i.a,{href:"https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md",children:["Docker documentation by ",(0,t.jsx)(i.code,{children:"llama.cpp"})," team"]})]}),"\n",(0,t.jsxs)(i.li,{children:["Download pre-built binaries from the ",(0,t.jsx)(i.a,{href:"https://github.com/ggml-org/llama.cpp/releases",children:"releases page"})]}),"\n",(0,t.jsxs)(i.li,{children:["Build from source by ",(0,t.jsx)(i.a,{href:"https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md",children:"cloning this repository"})]}),"\n"]}),(0,t.jsxs)(i.p,{children:["Once installed, you can use ",(0,t.jsx)(i.code,{children:"llama.cpp"})," to interact with GGUF models in conversation mode as follows:"]}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-bash",children:"# Use a local model file\nllama-cli -m my_model.gguf -cnv\n\n# Or download and run a model directly from Hugging Face\nllama-cli -hf Qiskit/Qwen2.5-Coder-14B-Qiskit-GGUF -cnv\n"})}),(0,t.jsx)(i.p,{children:"You can also launch an OpenAI-compatible API server for the model in the following way:"}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-bash",children:"llama-server -hf Qiskit/Qwen2.5-Coder-14B-Qiskit-GGUF\n"})}),(0,t.jsx)(i.h4,{id:"advanced-parameters",children:"Advanced parameters"}),(0,t.jsxs)(i.p,{children:["With the ",(0,t.jsx)(i.code,{children:"llama-cli"})," program, you can control the model generation using command-line options. For example, you can provide an initial \u201csystem\u201d prompt using the ",(0,t.jsx)(i.code,{children:"-p/--prompt"})," flag. In conversation mode (",(0,t.jsx)(i.code,{children:"-cnv"}),"), this initial prompt acts as the system message. Otherwise, you can simply prepend any desired instruction to your prompt text. You can also adjust sampling parameters - for instance: temperature (",(0,t.jsx)(i.code,{children:"--temp"}),"), top-k (",(0,t.jsx)(i.code,{children:"--top-k"}),"), top-p (",(0,t.jsx)(i.code,{children:"--top-p"}),"), repetition penalty (",(0,t.jsx)(i.code,{children:"--repeat-penalty"}),"), and the seed to use (",(0,t.jsx)(i.code,{children:"--seed"}),"). The following is an example invocation using these options:"]}),(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-bash",children:'llama-cli -hf Qiskit/Qwen2.5-Coder-14B-Qiskit-GGUF \\\n  -p "You are a friendly assistant." -cnv \\\n  --temp 0.7 \\\n  --top-k 50 \\\n  --top-p 0.95 \\\n  --repeat-penalty 1.1 \\\n  --seed 42\n'})}),(0,t.jsxs)(i.p,{children:["To ensure proper functionality of our Qiskit models, we recommend using the system prompt provided in our HF GGUF repositories: system prompt for ",(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/mistral-small-3.2-24b-qiskit-GGUF/blob/main/system",children:"mistral-small-3.2-24b-qiskit-GGUF"}),", ",(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/Qwen2.5-Coder-14B-Qiskit-GGUF/blob/main/system",children:"Qwen2.5-Coder-14B-Qiskit-GGUF"}),", ",(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/granite-3.3-8b-qiskit-GGUF/blob/main/system",children:"granite-3.3-8b-qiskit-GGUF"}),", and ",(0,t.jsx)(i.a,{href:"https://huggingface.co/Qiskit/granite-3.2-8b-qiskit-GGUF/blob/main/system",children:"granite-3.2-8b-qiskit-GGUF."})]})]}),"\n",(0,t.jsxs)(d,{children:[(0,t.jsx)("summary",{children:"Manually connect extensions to local deployment"}),(0,t.jsxs)(i.p,{children:["Use the VS Code extension and JupyterLab extension for the Qiskit Code Assistant to prompt the locally deployed Qiskit Code Assistant model. Once you have the Ollama application ",(0,t.jsx)(i.a,{href:"#using-the-ollama-application",children:"set up with the model"}),", you can configure the extensions to connect to the local service."]}),(0,t.jsx)(i.h3,{id:"connect-with-the-qiskit-code-assistant-vs-code-extension",children:"Connect with the Qiskit Code Assistant VS Code extension"}),(0,t.jsx)(i.p,{children:"With the Qiskit Code Assistant VS Code extension, you can interact with the model and perform code completion while writing your code. This can work well for users looking for assistance writing Qiskit code for their Python applications."}),(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["Install the ",(0,t.jsx)(i.a,{href:"/guides/qiskit-code-assistant-vscode",children:"Qiskit Code Assistant VS Code extension"}),"."]}),"\n",(0,t.jsxs)(i.li,{children:["In VS Code, go to the ",(0,t.jsx)(i.strong,{children:"User Settings"})," and set the ",(0,t.jsx)(i.strong,{children:"Qiskit Code Assistant: Url"})," to the URL of your local Ollama deployment (for example, ",(0,t.jsx)(i.code,{children:"http://localhost:11434"}),")."]}),"\n",(0,t.jsxs)(i.li,{children:["Reload VS Code by going to ",(0,t.jsx)(i.strong,{children:"View > Command Palette..."})," and selecting ",(0,t.jsx)(i.strong,{children:"Developer: Reload Window"}),"."]}),"\n"]}),(0,t.jsx)(i.p,{children:"The Qiskit Code Assistant model configured in Ollama should appear in the status bar and is then ready to use."}),(0,t.jsx)(i.h3,{id:"connect-with-the-qiskit-code-assistant-jupyterlab-extension",children:"Connect with the Qiskit Code Assistant JupyterLab extension"}),(0,t.jsx)(i.p,{children:"With the Qiskit Code Assistant JupyterLab extension, you can interact with the model and perform code completion directly in your Jupyter Notebook. Users who predominantly work with Jupyter Notebooks can take advantage of this extension to further enhance their experience writing Qiskit code."}),(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["Install the ",(0,t.jsx)(i.a,{href:"/guides/qiskit-code-assistant-jupyterlab",children:"Qiskit Code Assistant JupyterLab extension"}),"."]}),"\n",(0,t.jsxs)(i.li,{children:["In JupyterLab, go to the ",(0,t.jsx)(i.strong,{children:"Settings Editor"})," and set the ",(0,t.jsx)(i.strong,{children:"Qiskit Code Assistant Service API"})," to the URL of your local Ollama deployment (for example, ",(0,t.jsx)(i.code,{children:"http://localhost:11434"}),")."]}),"\n"]}),(0,t.jsx)(i.p,{children:"The Qiskit Code Assistant model configured in Ollama should appear in the status bar and is then ready to use."})]})]})}function h(e={}){const{wrapper:i}={...(0,l.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}function p(e,i){throw new Error("Expected "+(i?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},28453(e,i,n){n.d(i,{R:()=>d,x:()=>o});var s=n(96540);const t={},l=s.createContext(t);function d(e){const i=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:d(e.components),s.createElement(l.Provider,{value:i},e.children)}}}]);